\relax 
\providecommand\hyper@newdestlabel[2]{}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{alphamod}
\providecommand \oddpage@label [2]{}
\citation{graph_edit_distance}
\citation{graph_edit_distance}
\citation{ecd_ctxindependentsvm}
\citation{iec60617}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}\protected@file@percent }
\AC@undonewlabel{acro:ECD}
\newlabel{acro:ECD}{{1.1}{1}{Motivation}{section*.2}{}}
\acronymused{ECD}
\acronymused{ECD}
\AC@undonewlabel{acro:CAD}
\newlabel{acro:CAD}{{1.1}{1}{Motivation}{section*.3}{}}
\acronymused{CAD}
\acronymused{CAD}
\acronymused{ECD}
\AC@undonewlabel{acro:ECC}
\newlabel{acro:ECC}{{1.1}{1}{Motivation}{section*.4}{}}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{CAD}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A drawing of an \ac {ECD} on a grid paper background with its \acp {ECC} and annotations.\relax }}{2}{figure.caption.5}\protected@file@percent }
\acronymused{ECD}
\acronymused{ECC}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example_ecd}{{1.1}{2}{A drawing of an \ac {ECD} on a grid paper background with its \acp {ECC} and annotations.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Statement}{2}{section.1.2}\protected@file@percent }
\newlabel{sec:problem_statement}{{1.2}{2}{Problem Statement}{section.1.2}{}}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECC}
\citation{ecd_basecnn}
\citation{ecd_anngeo}
\citation{ecd_texturesmo}
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECD}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Related Work}{3}{section.1.3}\protected@file@percent }
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECD}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Classification}{3}{subsection.1.3.1}\protected@file@percent }
\newlabel{sec:rel_classification}{{1.3.1}{3}{Classification}{subsection.1.3.1}{}}
\AC@undonewlabel{acro:CNN}
\newlabel{acro:CNN}{{1.3.1}{3}{Classification}{section*.6}{}}
\acronymused{CNN}
\acronymused{ECC}
\acronymused{CNN}
\acronymused{ECC}
\AC@undonewlabel{acro:MLP}
\newlabel{acro:MLP}{{1.3.1}{3}{Classification}{section*.7}{}}
\acronymused{MLP}
\acronymused{ECC}
\AC@undonewlabel{acro:SMO}
\newlabel{acro:SMO}{{1.3.1}{3}{Classification}{section*.8}{}}
\acronymused{SMO}
\citation{ecd_knn_recog}
\citation{ecd_seghogsvm}
\citation{ecd_knnfull}
\acronymused{SMO}
\acronymused{MLP}
\AC@undonewlabel{acro:KNN}
\newlabel{acro:KNN}{{1.3.1}{4}{Classification}{section*.9}{}}
\acronymused{KNN}
\acronymused{SMO}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Extraction and Classification}{4}{subsection.1.3.2}\protected@file@percent }
\newlabel{sec:rel_extract_classification}{{1.3.2}{4}{Extraction and Classification}{subsection.1.3.2}{}}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECC}
\acronymused{KNN}
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECC}
\AC@undonewlabel{acro:HOG}
\newlabel{acro:HOG}{{1.3.2}{4}{Extraction and Classification}{section*.10}{}}
\acronymused{HOG}
\AC@undonewlabel{acro:SVM}
\newlabel{acro:SVM}{{1.3.2}{4}{Extraction and Classification}{section*.11}{}}
\acronymused{SVM}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Extraction, Classification and Topology Creation}{4}{subsection.1.3.3}\protected@file@percent }
\newlabel{sec:rel_extract_classification_topology}{{1.3.3}{4}{Extraction, Classification and Topology Creation}{subsection.1.3.3}{}}
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECC}
\citation{ecd_ctxindependentsvm}
\citation{line_primitive}
\citation{ecd_yolobool}
\citation{tensorflow}
\citation{hough_transform}
\acronymused{ECC}
\acronymused{KNN}
\acronymused{ECC}
\acronymused{SVM}
\acronymused{SVM}
\AC@undonewlabel{acro:YOLO}
\newlabel{acro:YOLO}{{1.3.3}{5}{Extraction, Classification and Topology Creation}{section*.12}{}}
\acronymused{YOLO}
\citation{tesseract}
\acronymused{YOLO}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Goals of this Thesis}{6}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Task Description}{6}{subsection.1.4.1}\protected@file@percent }
\acronymused{ECC}
\acronymused{YOLO}
\AC@undonewlabel{acro:MUnet}
\newlabel{acro:MUnet}{{1.4.1}{6}{Task Description}{section*.13}{}}
\acronymused{MUnet}
\acronymused{ECC}
\AC@undonewlabel{acro:OCR}
\newlabel{acro:OCR}{{1.4.1}{6}{Task Description}{section*.14}{}}
\acronymused{OCR}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Contribution}{6}{subsection.1.4.2}\protected@file@percent }
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECC}
\AC@undonewlabel{acro:YOLOv4}
\newlabel{acro:YOLOv4}{{1.4.2}{6}{Contribution}{section*.15}{}}
\acronymused{YOLOv4}
\acronymused{ECD}
\acronymused{MUnet}
\citation{iec60617}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theory}{9}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Electrical Circuit Diagrams}{9}{section.2.1}\protected@file@percent }
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECD}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces All used \acp {ECC} in this thesis in German notation: 1. Voltage Source, 2. Current Source, 3. Resistor, 4. Inductor, 5. Capacitor, 6. Diode, 7. Ground\relax }}{9}{figure.caption.16}\protected@file@percent }
\acronymused{ECC}
\newlabel{fig:used_eccs}{{2.1}{9}{All used \acp {ECC} in this thesis in German notation: 1. Voltage Source, 2. Current Source, 3. Resistor, 4. Inductor, 5. Capacitor, 6. Diode, 7. Ground\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}LTspice}{10}{section.2.2}\protected@file@percent }
\newlabel{sec:ltspice}{{2.2}{10}{LTspice}{section.2.2}{}}
\acronymused{ECD}
\@writefile{toc}{\contentsline {subsubsection}{Header}{10}{section*.17}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces LTspice header syntax\relax }}{10}{table.2.1}\protected@file@percent }
\newlabel{tab:ltheader_syntax}{{2.1}{10}{LTspice header syntax\relax }{table.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Symbols}{10}{section*.18}\protected@file@percent }
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces LTspice symbol names\relax }}{11}{table.2.2}\protected@file@percent }
\newlabel{tab:ltsymbol_mapping}{{2.2}{11}{LTspice symbol names\relax }{table.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces LTspice symbol syntax\relax }}{11}{table.2.3}\protected@file@percent }
\newlabel{tab:ltsymbol_syntax}{{2.3}{11}{LTspice symbol syntax\relax }{table.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Symbol Attributes}{11}{section*.19}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces LTspice symbol attribute syntax\relax }}{11}{table.2.4}\protected@file@percent }
\newlabel{tab:ltsymattr_syntax}{{2.4}{11}{LTspice symbol attribute syntax\relax }{table.2.4}{}}
\citation{bioneuron}
\citation{perceptron}
\@writefile{toc}{\contentsline {subsubsection}{Ground}{12}{section*.20}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces LTspice ground syntax\relax }}{12}{table.2.5}\protected@file@percent }
\newlabel{tab:ltflag_syntax}{{2.5}{12}{LTspice ground syntax\relax }{table.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Wire}{12}{section*.21}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces LTspice wire syntax\relax }}{12}{table.2.6}\protected@file@percent }
\newlabel{tab:ltwire_syntax}{{2.6}{12}{LTspice wire syntax\relax }{table.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Artificial Neural Networks}{12}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}General Concepts}{12}{subsection.2.3.1}\protected@file@percent }
\newlabel{sec:deep_basics}{{2.3.1}{12}{General Concepts}{subsection.2.3.1}{}}
\AC@undonewlabel{acro:ANN}
\newlabel{acro:ANN}{{2.3.1}{12}{General Concepts}{section*.22}{}}
\acronymused{ANN}
\citation{mlp}
\citation{dl}
\citation{dl_mit}
\citation{dl}
\citation{dl_mit}
\citation{softmax}
\newlabel{form:perceptron}{{2.1}{13}{General Concepts}{equation.2.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multilayer Perceptron}{13}{section*.23}\protected@file@percent }
\acronymused{MLP}
\acronymused{MLP}
\newlabel{eq:identity}{{2.2}{13}{Multilayer Perceptron}{equation.2.3.2}{}}
\AC@undonewlabel{acro:FC}
\newlabel{acro:FC}{{2.3.1}{13}{Multilayer Perceptron}{section*.24}{}}
\acronymused{FC}
\newlabel{eq:fc_weights}{{2.4}{13}{Multilayer Perceptron}{equation.2.3.4}{}}
\citation{one_hot_enc}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A MLP with two hidden layers (two neurons in the first and one in the last), each input gets multiplied with each weight of each neuron, summed up and fed into an activation function to produce the input for the next layer, where this process is repeated.\relax }}{14}{figure.caption.25}\protected@file@percent }
\newlabel{fig:mlp}{{2.2}{14}{A MLP with two hidden layers (two neurons in the first and one in the last), each input gets multiplied with each weight of each neuron, summed up and fed into an activation function to produce the input for the next layer, where this process is repeated.\relax }{figure.caption.25}{}}
\newlabel{eq:softmax}{{2.5}{14}{Multilayer Perceptron}{equation.2.3.5}{}}
\acronymused{MLP}
\@writefile{toc}{\contentsline {subsubsection}{Learning Procedure}{14}{section*.26}\protected@file@percent }
\citation{yolov1}
\citation{loss_function_segmentation}
\citation{sgd_momentum}
\citation{adam}
\citation{dl}
\AC@undonewlabel{acro:MSE}
\newlabel{acro:MSE}{{2.3.1}{15}{Learning Procedure}{section*.27}{}}
\acronymused{MSE}
\newlabel{eq:mse}{{2.6}{15}{Learning Procedure}{equation.2.3.6}{}}
\AC@undonewlabel{acro:CE}
\newlabel{acro:CE}{{2.3.1}{15}{Learning Procedure}{section*.28}{}}
\acronymused{CE}
\newlabel{eq:ce}{{2.7}{15}{Learning Procedure}{equation.2.3.7}{}}
\acronymused{MLP}
\citation{mish}
\citation{dl}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Activation Functions}{16}{subsection.2.3.2}\protected@file@percent }
\newlabel{sec:activation_functions}{{2.3.2}{16}{Activation Functions}{subsection.2.3.2}{}}
\acronymused{ANN}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid}{16}{section*.29}\protected@file@percent }
\acronymused{ANN}
\acronymused{ANN}
\newlabel{eq:sigmoid}{{2.11}{16}{Sigmoid}{equation.2.3.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{Rectified Linear Unit (ReLU)}{16}{section*.30}\protected@file@percent }
\AC@undonewlabel{acro:ReLU}
\newlabel{acro:ReLU}{{2.3.2}{16}{Rectified Linear Unit (ReLU)}{section*.31}{}}
\acronymused{ReLU}
\acronymused{ReLU}
\acronymused{ReLU}
\AC@undonewlabel{acro:LReLU}
\newlabel{acro:LReLU}{{2.3.2}{16}{Rectified Linear Unit (ReLU)}{section*.32}{}}
\acronymused{LReLU}
\acronymused{LReLU}
\acronymused{ReLU}
\citation{mnetv1}
\citation{lecun_lenet}
\citation{inception}
\citation{resnet}
\citation{densenet}
\citation{dl}
\@writefile{toc}{\contentsline {subsubsection}{ReLU6}{17}{section*.33}\protected@file@percent }
\acronymused{ReLU}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Convolutional Neural Networks}{17}{subsection.2.3.3}\protected@file@percent }
\acronymused{MLP}
\acronymused{ANN}
\acronymused{CNN}
\acronymused{CNN}
\@writefile{toc}{\contentsline {subsubsection}{Convolutional Layer}{17}{section*.34}\protected@file@percent }
\acronymused{CNN}
\newlabel{eq:conv}{{2.15}{17}{Convolutional Layer}{equation.2.3.15}{}}
\citation{conv_arithmetic}
\citation{conv_arithmetic}
\citation{dl}
\citation{batchnorm}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example convolution of a 4x4 input (blue) with a 3x3 kernel (dark blue) and a stride of 1, resulting in a 2x2 output (cyan) \cite  {conv_arithmetic}\relax }}{18}{figure.caption.35}\protected@file@percent }
\newlabel{fig:conv_example}{{2.3}{18}{Example convolution of a 4x4 input (blue) with a 3x3 kernel (dark blue) and a stride of 1, resulting in a 2x2 output (cyan) \cite {conv_arithmetic}\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{Pooling Layer}{18}{section*.36}\protected@file@percent }
\newlabel{sec:max_pooling}{{2.3.3}{18}{Pooling Layer}{section*.36}{}}
\acronymused{CNN}
\@writefile{toc}{\contentsline {subsubsection}{Fully-Connected Layer}{18}{section*.37}\protected@file@percent }
\acronymused{FC}
\acronymused{CNN}
\acronymused{FC}
\citation{augmentation_survey}
\citation{copypaste_aug}
\citation{albumentation}
\citation{pytorch}
\citation{tensorflow}
\citation{albumentation}
\citation{albumentation}
\citation{tta_segmentation_cells}
\citation{when_tta_works}
\citation{when_tta_works}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Batch Normalization}{19}{subsection.2.3.4}\protected@file@percent }
\acronymused{ReLU}
\acronymused{ReLU}
\AC@undonewlabel{acro:BatchNorm}
\newlabel{acro:BatchNorm}{{2.3.4}{19}{Batch Normalization}{section*.38}{}}
\acronymused{BatchNorm}
\acronymused{ANN}
\acronymused{BatchNorm}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Data Augmentation}{19}{section.2.4}\protected@file@percent }
\acronymused{CNN}
\acronymused{ECD}
\AC@undonewlabel{acro:TTA}
\newlabel{acro:TTA}{{2.4}{19}{Data Augmentation}{section*.39}{}}
\acronymused{TTA}
\acronymused{TTA}
\citation{yolov1}
\citation{sliding_window_satelite}
\@writefile{lot}{\contentsline {table}{\numberline {2.7}{\ignorespaces A listing of the different augmentations used from albumentations \cite  {albumentation} in this thesis and the target domain where they were applied to.\relax }}{20}{table.2.7}\protected@file@percent }
\newlabel{tab:used_augmentations}{{2.7}{20}{A listing of the different augmentations used from albumentations \cite {albumentation} in this thesis and the target domain where they were applied to.\relax }{table.2.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Object Detection}{20}{section.2.5}\protected@file@percent }
\newlabel{sec:object_detection}{{2.5}{20}{Object Detection}{section.2.5}{}}
\citation{rcnn}
\citation{selective_search}
\citation{bbox_regressor}
\citation{selective_search}
\citation{selective_search}
\citation{fast_rcnn}
\citation{rcnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}History of Object Detection}{21}{subsection.2.5.1}\protected@file@percent }
\newlabel{sec:hostory_obj_detection}{{2.5.1}{21}{History of Object Detection}{subsection.2.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sliding Window}{21}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Regions with CNN Features (R-CNN)}{21}{section*.41}\protected@file@percent }
\AC@undonewlabel{acro:R-CNN}
\newlabel{acro:R-CNN}{{2.5.1}{21}{Regions with CNN Features (R-CNN)}{section*.42}{}}
\acronymused{R-CNN}
\acronymused{CNN}
\acronymused{CNN}
\acronymused{SVM}
\acronymused{SVM}
\@writefile{toc}{\contentsline {subsubsection}{Fast R-CNN}{21}{section*.44}\protected@file@percent }
\acronymused{R-CNN}
\acronymused{R-CNN}
\acronymused{R-CNN}
\acronymused{CNN}
\acronymused{CNN}
\AC@undonewlabel{acro:RoI}
\newlabel{acro:RoI}{{2.5.1}{21}{Fast R-CNN}{section*.45}{}}
\acronymused{RoI}
\acronymused{CNN}
\acronymused{RoI}
\acronymused{RoI}
\citation{faster_rcnn}
\citation{yolov1}
\citation{ssd}
\citation{focalloss}
\citation{yolov3}
\citation{faster_rcnn}
\citation{yolov2}
\citation{yolov1}
\citation{corner_net}
\citation{center_net}
\citation{fcos}
\citation{fcos}
\citation{center_net}
\citation{yolov4}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Example of results obtained through the Selective Search algorithm (top) with increasing region scale from left to right and bounding boxes drawn around those regions (bottom). Selective Search produces sub-segmentations of objects in an image, considering size, color, texture and shape based features for the grouping of the regions \cite  {selective_search}.\relax }}{22}{figure.caption.43}\protected@file@percent }
\newlabel{fig:selective_search}{{2.4}{22}{Example of results obtained through the Selective Search algorithm (top) with increasing region scale from left to right and bounding boxes drawn around those regions (bottom). Selective Search produces sub-segmentations of objects in an image, considering size, color, texture and shape based features for the grouping of the regions \cite {selective_search}.\relax }{figure.caption.43}{}}
\acronymused{SVM}
\acronymused{R-CNN}
\@writefile{toc}{\contentsline {subsubsection}{Anchor Box Based Single Shot Detectors}{22}{section*.46}\protected@file@percent }
\acronymused{R-CNN}
\acronymused{YOLO}
\AC@undonewlabel{acro:SSD}
\newlabel{acro:SSD}{{2.5.1}{22}{Anchor Box Based Single Shot Detectors}{section*.47}{}}
\acronymused{SSD}
\citation{giou}
\citation{iou}
\citation{iou}
\citation{giou}
\citation{eiou}
\@writefile{toc}{\contentsline {subsubsection}{Anchor Box Free Single Shot Detectors}{23}{section*.48}\protected@file@percent }
\AC@undonewlabel{acro:YOLOv1}
\newlabel{acro:YOLOv1}{{2.5.1}{23}{Anchor Box Free Single Shot Detectors}{section*.49}{}}
\acronymused{YOLOv1}
\AC@undonewlabel{acro:FCOS}
\newlabel{acro:FCOS}{{2.5.1}{23}{Anchor Box Free Single Shot Detectors}{section*.50}{}}
\acronymused{FCOS}
\AC@undonewlabel{acro:IoU}
\newlabel{acro:IoU}{{2.5.1}{23}{Anchor Box Free Single Shot Detectors}{section*.51}{}}
\acronymused{IoU}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Intersection Over Union (IoU)}{23}{subsection.2.5.2}\protected@file@percent }
\acronymused{IoU}
\acronymused{IoU}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}IoU Based Loss Functions}{23}{subsection.2.5.3}\protected@file@percent }
\acronymused{MSE}
\acronymused{IoU}
\acronymused{IoU}
\newlabel{eq:iou_loss}{{2.18}{23}{IoU Based Loss Functions}{equation.2.5.18}{}}
\AC@undonewlabel{acro:GIoU}
\newlabel{acro:GIoU}{{2.5.3}{23}{IoU Based Loss Functions}{section*.52}{}}
\acronymused{GIoU}
\acronymused{IoU}
\acronymused{GIoU}
\citation{diou}
\citation{eiou}
\citation{eiou}
\acronymused{GIoU}
\acronymused{IoU}
\acronymused{GIoU}
\newlabel{eq:giou_loss}{{2.19}{24}{IoU Based Loss Functions}{equation.2.5.19}{}}
\acronymused{IoU}
\AC@undonewlabel{acro:DIoU}
\newlabel{acro:DIoU}{{2.5.3}{24}{IoU Based Loss Functions}{section*.53}{}}
\acronymused{DIoU}
\AC@undonewlabel{acro:CIoU}
\newlabel{acro:CIoU}{{2.5.3}{24}{IoU Based Loss Functions}{section*.54}{}}
\acronymused{CIoU}
\acronymused{GIoU}
\acronymused{DIoU}
\newlabel{eq:diou_loss}{{2.20}{24}{IoU Based Loss Functions}{equation.2.5.20}{}}
\acronymused{DIoU}
\acronymused{CIoU}
\acronymused{CIoU}
\newlabel{eq:ciou_loss}{{2.21}{24}{IoU Based Loss Functions}{equation.2.5.21}{}}
\newlabel{eq:ciou_nu}{{2.22}{24}{IoU Based Loss Functions}{equation.2.5.22}{}}
\newlabel{eq:ciou_alpha}{{2.23}{24}{IoU Based Loss Functions}{equation.2.5.23}{}}
\AC@undonewlabel{acro:EIoU}
\newlabel{acro:EIoU}{{2.5.3}{24}{IoU Based Loss Functions}{section*.55}{}}
\acronymused{EIoU}
\citation{yolov1}
\citation{yolov4}
\citation{yolov4_tiny}
\newlabel{eq:eiou_loss}{{2.24}{25}{IoU Based Loss Functions}{equation.2.5.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}You Only Look Once (YOLO)}{25}{section.2.6}\protected@file@percent }
\newlabel{sec:yolo}{{2.6}{25}{You Only Look Once (YOLO)}{section.2.6}{}}
\acronymused{YOLO}
\acronymused{YOLO}
\acronymused{YOLO}
\acronymused{YOLOv4}
\acronymused{YOLOv4}
\@writefile{toc}{\contentsline {subsubsection}{General}{25}{section*.56}\protected@file@percent }
\acronymused{YOLOv4}
\@writefile{toc}{\contentsline {subsubsection}{Backbone}{25}{section*.57}\protected@file@percent }
\acronymused{YOLOv4}
\acronymused{ReLU}
\citation{yolov4}
\citation{yolov4}
\citation{pannet}
\citation{yolov4_tiny}
\citation{pannet}
\citation{pannet}
\citation{yolov3}
\@writefile{lot}{\contentsline {table}{\numberline {2.8}{\ignorespaces Convolutional basic building block in YOLOv4 (YOLOConv). A convolutional layer, followed by a batch normalization layer, followed by a LeakyReLU activation function.\relax }}{26}{table.2.8}\protected@file@percent }
\newlabel{tab:yoloconv}{{2.8}{26}{Convolutional basic building block in YOLOv4 (YOLOConv). A convolutional layer, followed by a batch normalization layer, followed by a LeakyReLU activation function.\relax }{table.2.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Neck}{26}{section*.58}\protected@file@percent }
\acronymused{YOLOv4}
\AC@undonewlabel{acro:PANet}
\newlabel{acro:PANet}{{2.6}{26}{Neck}{section*.59}{}}
\acronymused{PANet}
\@writefile{lot}{\contentsline {table}{\numberline {2.9}{\ignorespaces The CSPDarknet53Tiny Architecture is a scaled version of the original CSPDarknet53 architecture \cite  {yolov4}. The definition for YOLOConv can be found in table \ref  {tab:yoloconv}. MaxPool indicates here the Max Pooling operation as described in sec. \ref  {sec:max_pooling}. The network is build out of five blocks, the first block reducing the image size and the following blocks building up features. Further, features from three different scales are taken and used as skip connections to the following network.\relax }}{27}{table.2.9}\protected@file@percent }
\newlabel{tab:darknet_tiny_arch}{{2.9}{27}{The CSPDarknet53Tiny Architecture is a scaled version of the original CSPDarknet53 architecture \cite {yolov4}. The definition for YOLOConv can be found in table \ref {tab:yoloconv}. MaxPool indicates here the Max Pooling operation as described in sec. \ref {sec:max_pooling}. The network is build out of five blocks, the first block reducing the image size and the following blocks building up features. Further, features from three different scales are taken and used as skip connections to the following network.\relax }{table.2.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.10}{\ignorespaces The PANetTiny architecture which is a scaled version of PANet \cite  {pannet}, which is the decoder in the YOLO network. It takes as inputs the skip connections from the CSPDarknet53Tiny. The network is build by alternating an output branch and an upsampling branch. First, the lowest skip connection $\mathbf  {Skip_L}$ from the backbone is convolved and the output fed into the first output layer. Each output layer has again a $3\times 3$ convolution followed by a $1\times 1$ convolution, which form a raw bounding box prediction, fed to the YOLO head. After an output has been processed the previous convolution is again convolved and upsampled to be fed into the next output block. This is done three times in the whole PANetTiny network.\relax }}{28}{table.2.10}\protected@file@percent }
\newlabel{tab:panet_tiny_arch}{{2.10}{28}{The PANetTiny architecture which is a scaled version of PANet \cite {pannet}, which is the decoder in the YOLO network. It takes as inputs the skip connections from the CSPDarknet53Tiny. The network is build by alternating an output branch and an upsampling branch. First, the lowest skip connection $\mathbf {Skip_L}$ from the backbone is convolved and the output fed into the first output layer. Each output layer has again a $3\times 3$ convolution followed by a $1\times 1$ convolution, which form a raw bounding box prediction, fed to the YOLO head. After an output has been processed the previous convolution is again convolved and upsampled to be fed into the next output block. This is done three times in the whole PANetTiny network.\relax }{table.2.10}{}}
\citation{yolov2}
\citation{fast_rcnn}
\citation{faster_rcnn}
\citation{yolov2}
\citation{fast_rcnn}
\citation{faster_rcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Bounding box calculation in the YOLOv4 network based on a prior anchor box \cite  {yolov2}. The $b_*$ indicate the final prediction, $p_*$ indicate parameters of the prior bounding box and $c_*$ indicate the prior bounding box spatial offset based on the current cell. The final center coordinate offset is predicted non-linearly with a sigmoid function, while the final width and height are predicted linearly, as it is done in \cite  {fast_rcnn} and \cite  {faster_rcnn}.\relax }}{29}{figure.caption.61}\protected@file@percent }
\newlabel{fig:bbox_calculation}{{2.5}{29}{Bounding box calculation in the YOLOv4 network based on a prior anchor box \cite {yolov2}. The $b_*$ indicate the final prediction, $p_*$ indicate parameters of the prior bounding box and $c_*$ indicate the prior bounding box spatial offset based on the current cell. The final center coordinate offset is predicted non-linearly with a sigmoid function, while the final width and height are predicted linearly, as it is done in \cite {fast_rcnn} and \cite {faster_rcnn}.\relax }{figure.caption.61}{}}
\@writefile{toc}{\contentsline {subsubsection}{Head}{29}{section*.60}\protected@file@percent }
\acronymused{YOLOv4}
\@writefile{toc}{\contentsline {subsubsection}{Loss}{29}{section*.62}\protected@file@percent }
\acronymused{YOLO}
\acronymused{IoU}
\acronymused{YOLOv4}
\acronymused{CE}
\newlabel{eq:yolo_lclass}{{2.26}{30}{Loss}{equation.2.6.26}{}}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{GIoU}
\acronymused{DIoU}
\acronymused{CIoU}
\acronymused{EIoU}
\newlabel{eq:yolo_lbbox}{{2.27}{30}{Loss}{equation.2.6.27}{}}
\acronymused{CE}
\acronymused{IoU}
\citation{diou}
\citation{soft_nms}
\acronymused{IoU}
\acronymused{IoU}
\newlabel{eq:yolo_lobj}{{2.29}{31}{Loss}{equation.2.6.29}{}}
\newlabel{eq:yolo_lnoobj}{{2.30}{31}{Loss}{equation.2.6.30}{}}
\acronymused{YOLO}
\newlabel{eq:yolo_loss}{{2.31}{31}{Loss}{equation.2.6.31}{}}
\@writefile{toc}{\contentsline {subsubsection}{Non-Maximum Suppression (NMS)}{31}{section*.63}\protected@file@percent }
\AC@undonewlabel{acro:NMS}
\newlabel{acro:NMS}{{2.6}{31}{Non-Maximum Suppression (NMS)}{section*.64}{}}
\acronymused{NMS}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{NMS}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{DIoU}
\citation{weighted_bbox_fusion}
\acronymused{DIoU}
\acronymused{NMS}
\@writefile{loa}{\contentsline {algocf}{\numberline {2.1}{\ignorespaces \ac {DIoU}-\ac {NMS}\relax }}{32}{algocf.2.1}\protected@file@percent }
\newlabel{alg:diou_nms}{{2.1}{32}{Non-Maximum Suppression (NMS)}{algocf.2.1}{}}
\AC@undonewlabel{acro:WBF}
\newlabel{acro:WBF}{{12}{32}{Non-Maximum Suppression (NMS)}{section*.65}{}}
\acronymused{WBF}
\acronymused{IoU}
\citation{semantic_segmentation}
\citation{mask_rcnn}
\acronymused{TTA}
\newlabel{eq:confidence_rescale}{{2.35}{33}{Non-Maximum Suppression (NMS)}{equation.2.6.35}{}}
\newlabel{eq:confidence_rescale_not_used}{{2.36}{33}{Non-Maximum Suppression (NMS)}{equation.2.6.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Segmentation}{33}{section.2.7}\protected@file@percent }
\newlabel{sec:segmentation}{{2.7}{33}{Segmentation}{section.2.7}{}}
\citation{instance_vs_semantic_fig}
\citation{instance_vs_semantic_fig}
\citation{mobile_unet}
\citation{unet}
\citation{mnetv2}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The difference between object detection, semantic segmentation and instance segmentation. In object detection the instance with a rough estimate (bounding box) is predicted, in semantic segmentation a segmentation mask for an object is predicted without considering the underlying instance and in instance segmentation the instance as well as a segmentation mask for an object is predicted \cite  {instance_vs_semantic_fig}.\relax }}{34}{figure.caption.66}\protected@file@percent }
\newlabel{fig:instance_vs_semantic}{{2.6}{34}{The difference between object detection, semantic segmentation and instance segmentation. In object detection the instance with a rough estimate (bounding box) is predicted, in semantic segmentation a segmentation mask for an object is predicted without considering the underlying instance and in instance segmentation the instance as well as a segmentation mask for an object is predicted \cite {instance_vs_semantic_fig}.\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}MobileNetV2-UNet}{34}{section.2.8}\protected@file@percent }
\newlabel{sec:mobilenetv2_unet}{{2.8}{34}{MobileNetV2-UNet}{section.2.8}{}}
\acronymused{MUnet}
\acronymused{CNN}
\acronymused{YOLOv4}
\acronymused{MUnet}
\acronymused{MUnet}
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV2-UNet backbone}{34}{section*.67}\protected@file@percent }
\acronymused{MUnet}
\citation{mnetv1}
\citation{mnetv2}
\citation{mnetv2}
\citation{unet}
\@writefile{lot}{\contentsline {table}{\numberline {2.11}{\ignorespaces An inverted residual block transforming from $k$ to $k'$ channels, with stride $s$ and expansion factor $t$.\relax }}{35}{table.2.11}\protected@file@percent }
\newlabel{tab:invres_expansion}{{2.11}{35}{An inverted residual block transforming from $k$ to $k'$ channels, with stride $s$ and expansion factor $t$.\relax }{table.2.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV2-UNet decoder}{35}{section*.69}\protected@file@percent }
\acronymused{MUnet}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces (a) The inverted residual block. A $1 \times 1$ convolution with non-linear activation followed by a depthwise separable convolution and a linear $1 \times 1$ convolution with residual connection to the input. The residual connection is here additive, i.e. the input gets added to the output of the linear $1 \times 1$ convolution. It is called inverted, due to the expansion inside the block, while in the traditional residual block (b) the input gets expanded when it leaves the block \cite  {mnetv2}.\relax }}{36}{figure.caption.68}\protected@file@percent }
\newlabel{fig:inverted_residual}{{2.7}{36}{(a) The inverted residual block. A $1 \times 1$ convolution with non-linear activation followed by a depthwise separable convolution and a linear $1 \times 1$ convolution with residual connection to the input. The residual connection is here additive, i.e. the input gets added to the output of the linear $1 \times 1$ convolution. It is called inverted, due to the expansion inside the block, while in the traditional residual block (b) the input gets expanded when it leaves the block \cite {mnetv2}.\relax }{figure.caption.68}{}}
\citation{hypergraph_def}
\citation{hypergraph_adjacency}
\citation{hypergraph_adjacency_my}
\citation{hypergraph_adjacency_my}
\citation{tpfp}
\@writefile{lot}{\contentsline {table}{\numberline {2.12}{\ignorespaces MobileNetV2 encoder used in \ac {MUnet} Blocks marked as $Skip_*$ are outputs from the network which are used in the decoder. The parameters above define the configuration of the respective block, $t$ is the expansion factor inside an inverted residual block as defined in \ref  {tab:invres_expansion}, $k$ is the kernel size for the two standard convolutions used in the backbone, $n$ defines how often that particular block is repeated and $s$ is the stride of a block. Note that if $s = 2$ only the first block has this stride, the others have $s = 1$.\relax }}{37}{table.2.12}\protected@file@percent }
\newlabel{tab:mobilenetv2_architecture}{{2.12}{37}{MobileNetV2 encoder used in \ac {MUnet} Blocks marked as $Skip_*$ are outputs from the network which are used in the decoder. The parameters above define the configuration of the respective block, $t$ is the expansion factor inside an inverted residual block as defined in \ref {tab:invres_expansion}, $k$ is the kernel size for the two standard convolutions used in the backbone, $n$ defines how often that particular block is repeated and $s$ is the stride of a block. Note that if $s = 2$ only the first block has this stride, the others have $s = 1$.\relax }{table.2.12}{}}
\acronymused{MUnet}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Hypergraphs}{37}{section.2.9}\protected@file@percent }
\acronymused{ECD}
\@writefile{lot}{\contentsline {table}{\numberline {2.13}{\ignorespaces \ac {MUnet} decoder. The decoder uses transpose convolutions to upsample the input (ConvTranspose) and as the backbone, inverted residuals to process the upsampled input together with the skip connection. The '+' indicates a concatenation along the channel axis. Since the first block in the backbone directly downsamples the input there is no skip connection with the size of the input. Therefore the last layer of the decoder is a upsampling layer, which uses a bilinear upsampling method to increase the size of the prediction to the size of the input. As with the backbone $t$ indicates the expansion size of the inverted residual block, $k$ indicates the used kernel size and $s$ indicates the used stride.\relax }}{38}{table.2.13}\protected@file@percent }
\newlabel{tab:mobilenetv2_decoder}{{2.13}{38}{\ac {MUnet} decoder. The decoder uses transpose convolutions to upsample the input (ConvTranspose) and as the backbone, inverted residuals to process the upsampled input together with the skip connection. The '+' indicates a concatenation along the channel axis. Since the first block in the backbone directly downsamples the input there is no skip connection with the size of the input. Therefore the last layer of the decoder is a upsampling layer, which uses a bilinear upsampling method to increase the size of the prediction to the size of the input. As with the backbone $t$ indicates the expansion size of the inverted residual block, $k$ indicates the used kernel size and $s$ indicates the used stride.\relax }{table.2.13}{}}
\acronymused{MUnet}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces An example drawing of a hypergraph with its corresponding adjacency matrix. The hypergraph is defined as: $\mathbf  {H} = (\mathbf  {V}, \mathbf  {E})$, $\mathbf  {V} = \{v_1, v_2, v_3, v_4\}$, $\mathbf  {E} = \{\mathbf  {e_1}, \mathbf  {e_2}\}$, with $\mathbf  {e_1} = \{v_1, v_2, v_3\}$, $\mathbf  {e_2} = \{v_3, v_4\}$. In the adjacency matrix a row corresponds to a hyperedge and each column to a vertex. When a vertex is present in a hyperedge it has a 1 as an entry in the matrix, when a vertex is not present it has a 0 \cite  {hypergraph_adjacency_my}.\relax }}{38}{figure.caption.70}\protected@file@percent }
\newlabel{fig:hypergraph_adjacency}{{2.8}{38}{An example drawing of a hypergraph with its corresponding adjacency matrix. The hypergraph is defined as: $\mathbf {H} = (\mathbf {V}, \mathbf {E})$, $\mathbf {V} = \{v_1, v_2, v_3, v_4\}$, $\mathbf {E} = \{\mathbf {e_1}, \mathbf {e_2}\}$, with $\mathbf {e_1} = \{v_1, v_2, v_3\}$, $\mathbf {e_2} = \{v_3, v_4\}$. In the adjacency matrix a row corresponds to a hyperedge and each column to a vertex. When a vertex is present in a hyperedge it has a 1 as an entry in the matrix, when a vertex is not present it has a 0 \cite {hypergraph_adjacency_my}.\relax }{figure.caption.70}{}}
\citation{map_coco}
\citation{map_pascal_voc}
\citation{map_coco}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Metrics}{39}{section.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{True Positive, False Positive, True Negative, False Negative}{39}{section*.71}\protected@file@percent }
\newlabel{sec:tpfpfn}{{2.10}{39}{True Positive, False Positive, True Negative, False Negative}{section*.71}{}}
\AC@undonewlabel{acro:TP}
\newlabel{acro:TP}{{2.10}{39}{True Positive, False Positive, True Negative, False Negative}{section*.72}{}}
\acronymused{TP}
\AC@undonewlabel{acro:FP}
\newlabel{acro:FP}{{2.10}{39}{True Positive, False Positive, True Negative, False Negative}{section*.73}{}}
\acronymused{FP}
\AC@undonewlabel{acro:TN}
\newlabel{acro:TN}{{2.10}{39}{True Positive, False Positive, True Negative, False Negative}{section*.74}{}}
\acronymused{TN}
\AC@undonewlabel{acro:FN}
\newlabel{acro:FN}{{2.10}{39}{True Positive, False Positive, True Negative, False Negative}{section*.75}{}}
\acronymused{FN}
\acronymused{TP}
\acronymused{TN}
\acronymused{FP}
\acronymused{FN}
\acronymused{IoU}
\acronymused{TP}
\acronymused{IoU}
\@writefile{toc}{\contentsline {subsubsection}{Precision}{39}{section*.76}\protected@file@percent }
\acronymused{TP}
\newlabel{eq:precision}{{2.37}{39}{Precision}{equation.2.10.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{Recall}{39}{section*.77}\protected@file@percent }
\newlabel{eq:recall}{{2.38}{39}{Recall}{equation.2.10.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{F1-Score}{39}{section*.78}\protected@file@percent }
\citation{map_article}
\citation{map_article}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Example of a precision-recall curve, where precision and recall were calculated for different \ac {IoU} thresholds and sorted and plotted by their recall values \cite  {map_article}. The \ac {AP} is the area under the curve.\relax }}{40}{figure.caption.81}\protected@file@percent }
\acronymused{IoU}
\acronymused{AP}
\newlabel{fig:pr_curve}{{2.9}{40}{Example of a precision-recall curve, where precision and recall were calculated for different \ac {IoU} thresholds and sorted and plotted by their recall values \cite {map_article}. The \ac {AP} is the area under the curve.\relax }{figure.caption.81}{}}
\@writefile{toc}{\contentsline {subsubsection}{Average Precision (AP)}{40}{section*.79}\protected@file@percent }
\AC@undonewlabel{acro:AP}
\newlabel{acro:AP}{{2.10}{40}{Average Precision (AP)}{section*.80}{}}
\acronymused{AP}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{AP}
\newlabel{eq:ap}{{2.40}{40}{Average Precision (AP)}{equation.2.10.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{Mean Average Precision (mAP)}{41}{section*.82}\protected@file@percent }
\acronymused{AP}
\AC@undonewlabel{acro:mAP}
\newlabel{acro:mAP}{{2.10}{41}{Mean Average Precision (mAP)}{section*.83}{}}
\acronymused{mAP}
\acronymused{AP}
\newlabel{eq:map}{{2.41}{41}{Mean Average Precision (mAP)}{equation.2.10.41}{}}
\@writefile{toc}{\contentsline {subsubsection}{Mean Intersection over Union (mIoU)}{41}{section*.84}\protected@file@percent }
\AC@undonewlabel{acro:mIoU}
\newlabel{acro:mIoU}{{2.10}{41}{Mean Intersection over Union (mIoU)}{section*.85}{}}
\acronymused{mIoU}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{mIoU}
\newlabel{eq:miou}{{2.42}{41}{Mean Intersection over Union (mIoU)}{equation.2.10.42}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Material and Methods}{43}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Data}{43}{section.3.1}\protected@file@percent }
\newlabel{sec:data}{{3.1}{43}{Data}{section.3.1}{}}
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECD}
\citation{labelme}
\citation{canny_edge}
\citation{opencv}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Amount of images of \acp {ECD} used in this thesis shown with their underlying background and whether they are annotated or not. Further, the train / validation / test split of the different image types is shown. While this might seem like a big split for test, the number of bounding boxes included in the test set is way smaller and is shown in table \ref  {tab:yolo_classes}. For the test dataset 10 persons were used, which are not present in the train and validation dataset.\relax }}{44}{table.3.1}\protected@file@percent }
\newlabel{tab:data_distribution}{{3.1}{44}{Amount of images of \acp {ECD} used in this thesis shown with their underlying background and whether they are annotated or not. Further, the train / validation / test split of the different image types is shown. While this might seem like a big split for test, the number of bounding boxes included in the test set is way smaller and is shown in table \ref {tab:yolo_classes}. For the test dataset 10 persons were used, which are not present in the train and validation dataset.\relax }{table.3.1}{}}
\acronymused{ECD}
\@writefile{toc}{\contentsline {subsubsection}{Labels: Object Detection and Segmentation}{44}{section*.86}\protected@file@percent }
\acronymused{YOLOv4}
\acronymused{MUnet}
\acronymused{YOLOv4}
\acronymused{MUnet}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{YOLO}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces The classes present in this thesis with their major class which is an \ac {ECC} and alternatively their orientation. Furthermore, the total amount of classes is shown and the train, valid, test ratio.\relax }}{45}{table.3.2}\protected@file@percent }
\newlabel{tab:yolo_classes}{{3.2}{45}{The classes present in this thesis with their major class which is an \ac {ECC} and alternatively their orientation. Furthermore, the total amount of classes is shown and the train, valid, test ratio.\relax }{table.3.2}{}}
\acronymused{ECC}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Example segmentation label mask (left) and bounding boxes labels (right).\relax }}{46}{figure.caption.87}\protected@file@percent }
\newlabel{fig:example_labels}{{3.1}{46}{Example segmentation label mask (left) and bounding boxes labels (right).\relax }{figure.caption.87}{}}
\@writefile{toc}{\contentsline {subsubsection}{Labels: Topology}{46}{section*.88}\protected@file@percent }
\newlabel{sec:hypergraph_topology}{{3.1}{46}{Labels: Topology}{section*.88}{}}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECD}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An example visual labeling helper image (top), shown with the index of the respective bounding box in the YOLO label file, together with the corresponding hypergraph adjacency matrix (bottom). Two columns in the matrix correspond to one \ac {ECC}, each column to one orientation, where the first column can either be the left or top orientation and the second column be the right or bottom orientation. The last column in the table further shows the string representation of an edge, which acts as the input for the labeling tool. For instance ``1r,0t,4t'' means that 1 right, 0 top and 4 top are connected to each other through a hyperedge.\relax }}{47}{figure.caption.89}\protected@file@percent }
\acronymused{ECC}
\newlabel{fig:example_topology_label}{{3.2}{47}{An example visual labeling helper image (top), shown with the index of the respective bounding box in the YOLO label file, together with the corresponding hypergraph adjacency matrix (bottom). Two columns in the matrix correspond to one \ac {ECC}, each column to one orientation, where the first column can either be the left or top orientation and the second column be the right or bottom orientation. The last column in the table further shows the string representation of an edge, which acts as the input for the labeling tool. For instance ``1r,0t,4t'' means that 1 right, 0 top and 4 top are connected to each other through a hyperedge.\relax }{figure.caption.89}{}}
\acronymused{ECC}
\acronymused{ECD}
\@writefile{toc}{\contentsline {subsubsection}{Labels: Textual Annotations and Arrow Annotations}{48}{section*.90}\protected@file@percent }
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\AC@undonewlabel{acro:CSV}
\newlabel{acro:CSV}{{3.1}{48}{Labels: Textual Annotations and Arrow Annotations}{section*.91}{}}
\acronymused{CSV}
\acronymused{ECC}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Recognition and Conversion Pipeline}{48}{section.3.2}\protected@file@percent }
\newlabel{sec:pipeline}{{3.2}{48}{Recognition and Conversion Pipeline}{section.3.2}{}}
\acronymused{ECD}
\acronymused{ECD}
\AC@undonewlabel{acro:IDom}
\newlabel{acro:IDom}{{3.2}{48}{Recognition and Conversion Pipeline}{section*.92}{}}
\acronymused{IDom}
\AC@undonewlabel{acro:LDom}
\newlabel{acro:LDom}{{3.2}{48}{Recognition and Conversion Pipeline}{section*.93}{}}
\acronymused{LDom}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\@writefile{toc}{\contentsline {subsubsection}{Stage 1: Object Detection}{49}{section*.95}\protected@file@percent }
\acronymused{YOLOv4}
\acronymused{YOLO}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{IDom}
\acronymused{YOLO}
\@writefile{toc}{\contentsline {subsubsection}{Stage 2: Segmentation}{49}{section*.96}\protected@file@percent }
\acronymused{MUnet}
\acronymused{IDom}
\acronymused{ECD}
\acronymused{MUnet}
\@writefile{toc}{\contentsline {subsubsection}{Stage 3: Topology Creation}{49}{section*.97}\protected@file@percent }
\acronymused{ECC}
\acronymused{IDom}
\acronymused{LDom}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The six-stage pipeline presented in this thesis. Stage 1 shows the results of the object detection with \ac {YOLO} and stage 2 the segmentation results with the \ac {MUnet}. Stage 3-5 are postprocessing stages, where the topology is created, the annotations are matched to their respective \ac {ECC} and the characters of the textual annotations are recognized. The last stage is the conversion stage, where the gathered information is embedded into the LTspice schematic file format.\relax }}{50}{figure.caption.94}\protected@file@percent }
\acronymused{YOLO}
\acronymused{MUnet}
\acronymused{ECC}
\newlabel{fig:pipeline}{{3.3}{50}{The six-stage pipeline presented in this thesis. Stage 1 shows the results of the object detection with \ac {YOLO} and stage 2 the segmentation results with the \ac {MUnet}. Stage 3-5 are postprocessing stages, where the topology is created, the annotations are matched to their respective \ac {ECC} and the characters of the textual annotations are recognized. The last stage is the conversion stage, where the gathered information is embedded into the LTspice schematic file format.\relax }{figure.caption.94}{}}
\citation{cca}
\citation{cv}
\acronymused{ECC}
\@writefile{toc}{\contentsline {subsubsection}{Stage 4: Annotation Matching}{52}{section*.98}\protected@file@percent }
\acronymused{ECC}
\acronymused{LDom}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{FP}
\@writefile{loa}{\contentsline {algocf}{\numberline {3.1}{\ignorespaces Arrow Annotation Matching\relax }}{53}{algocf.3.1}\protected@file@percent }
\newlabel{alg:arrow_matching}{{3.1}{53}{Stage 4: Annotation Matching}{algocf.3.1}{}}
\citation{tesseract}
\@writefile{loa}{\contentsline {algocf}{\numberline {3.2}{\ignorespaces Textual Annotation Matching\relax }}{54}{algocf.3.2}\protected@file@percent }
\newlabel{alg:text_matching}{{3.2}{54}{Stage 4: Annotation Matching}{algocf.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Stage 5: Optical Character Recognition of Textual Annotations}{54}{section*.99}\protected@file@percent }
\acronymused{OCR}
\@writefile{toc}{\contentsline {subsubsection}{Stage 6: LTspice Conversion}{54}{section*.100}\protected@file@percent }
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{LDom}
\acronymused{IDom}
\acronymused{IDom}
\acronymused{YOLO}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{IDom}
\acronymused{ECC}
\acronymused{IDom}
\acronymused{LDom}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{IDom}
\acronymused{LDom}
\acronymused{LDom}
\newlabel{eq:ldom_pos}{{3.1}{55}{Stage 6: LTspice Conversion}{equation.3.2.1}{}}
\acronymused{ECC}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Training and Experiments}{57}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\acronymused{YOLOv4}
\acronymused{MUnet}
\acronymused{ECD}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}YOLOv4-Tiny}{57}{section.4.1}\protected@file@percent }
\newlabel{sec:training_yolo}{{4.1}{57}{YOLOv4-Tiny}{section.4.1}{}}
\acronymused{YOLO}
\acronymused{YOLO}
\acronymused{YOLO}
\acronymused{CIoU}
\AC@undonewlabel{acro:SGD}
\newlabel{acro:SGD}{{\caption@xref {acro:SGD}{ on input line 19}}{57}{YOLOv4-Tiny}{section*.101}{}}
\acronymused{SGD}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The initial training configuration for the experiments performed with the YOLO network.\relax }}{57}{table.4.1}\protected@file@percent }
\newlabel{tab:initial_yolo_config}{{4.1}{57}{The initial training configuration for the experiments performed with the YOLO network.\relax }{table.4.1}{}}
\citation{yolov2}
\citation{yolov4}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces An overview of the conducted experiments with the \ac {YOLO}. Best parameters which were obtained from a previous experiment were used in the following experiment.\relax }}{58}{figure.caption.102}\protected@file@percent }
\acronymused{YOLO}
\newlabel{fig:yolo_experiments_overview}{{4.1}{58}{An overview of the conducted experiments with the \ac {YOLO}. Best parameters which were obtained from a previous experiment were used in the following experiment.\relax }{figure.caption.102}{}}
\acronymused{YOLO}
\acronymused{YOLOv4}
\acronymused{IoU}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Learning Rate Experiment}{58}{subsection.4.1.1}\protected@file@percent }
\newlabel{sec:yolo_lr}{{4.1.1}{58}{Learning Rate Experiment}{subsection.4.1.1}{}}
\citation{copypaste_aug}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The results of the initial learning rate search shown on the validation set, with the mean and standard deviation of the \acp {mAP} over three separate training runs.\relax }}{59}{figure.caption.103}\protected@file@percent }
\acronymused{mAP}
\newlabel{fig:yolo_lr_experiment_results}{{4.2}{59}{The results of the initial learning rate search shown on the validation set, with the mean and standard deviation of the \acp {mAP} over three separate training runs.\relax }{figure.caption.103}{}}
\acronymused{mAP}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Augmentation Experiments}{59}{subsection.4.1.2}\protected@file@percent }
\newlabel{sec:yolo_augs}{{4.1.2}{59}{Augmentation Experiments}{subsection.4.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Offline Augmentations}{59}{section*.104}\protected@file@percent }
\acronymused{AP}
\acronymused{mAP}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The results of the offline augmentation with the different offline augmentation configurations compared with the results of the best performing learning rate (baseline). When rotation and flip are enabled simultaneously the flipped image also gets rotated three times by 90\textdegree . Results are given with the mean and standard deviation of the \acp {mAP} of three separate training runs.\relax }}{60}{figure.caption.105}\protected@file@percent }
\acronymused{mAP}
\newlabel{fig:yolo_offline_aug_results}{{4.3}{60}{The results of the offline augmentation with the different offline augmentation configurations compared with the results of the best performing learning rate (baseline). When rotation and flip are enabled simultaneously the flipped image also gets rotated three times by 90\textdegree . Results are given with the mean and standard deviation of the \acp {mAP} of three separate training runs.\relax }{figure.caption.105}{}}
\acronymused{mAP}
\acronymused{ECC}
\acronymused{mAP}
\acronymused{ECC}
\acronymused{ECC}
\@writefile{toc}{\contentsline {subsubsection}{Online Augmentations}{60}{section*.107}\protected@file@percent }
\citation{eiou}
\acronymused{mAP}
\acronymused{mAP}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Results of the online augmentation experiment compared to the baseline which was established in the offline augmentation experiment. Each augmentation shows a clear increase in \ac {mAP} in comparison to the baseline. Results are given with the mean and standard deviation of the \acp {mAP} of three separate training runs.\relax }}{61}{figure.caption.106}\protected@file@percent }
\acronymused{mAP}
\acronymused{mAP}
\newlabel{fig:yolo_online_aug_results}{{4.4}{61}{Results of the online augmentation experiment compared to the baseline which was established in the offline augmentation experiment. Each augmentation shows a clear increase in \ac {mAP} in comparison to the baseline. Results are given with the mean and standard deviation of the \acp {mAP} of three separate training runs.\relax }{figure.caption.106}{}}
\citation{yolov2}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Grid Search Experiment on Hyperparameters}{62}{subsection.4.1.3}\protected@file@percent }
\newlabel{sec:yolo_grid}{{4.1.3}{62}{Grid Search Experiment on Hyperparameters}{subsection.4.1.3}{}}
\acronymused{CIoU}
\acronymused{EIoU}
\acronymused{EIoU}
\acronymused{EIoU}
\acronymused{EIoU}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Configuration of the YOLO grid search experiment with the tested grid parameters and the fixed offline and online augmentation parameters. The networks were trained for all possible configurations of learning rate, batch size and loss function. \relax }}{62}{table.4.2}\protected@file@percent }
\newlabel{tab:yolo_grid_search_config}{{4.2}{62}{Configuration of the YOLO grid search experiment with the tested grid parameters and the fixed offline and online augmentation parameters. The networks were trained for all possible configurations of learning rate, batch size and loss function. \relax }{table.4.2}{}}
\acronymused{EIoU}
\acronymused{CIoU}
\acronymused{EIoU}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Post-Training Fine-Tuning Experiments}{62}{subsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Input Size Tuning}{62}{section*.110}\protected@file@percent }
\acronymused{YOLOv4}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Results of the grid search shown for all loss functions, learning rates and batch sizes. Independent of the loss / learning rate combination a batch size of 32 performed consistently worse than a batch size of 64.\relax }}{63}{figure.caption.108}\protected@file@percent }
\newlabel{fig:yolo_grid_bs_compare_results}{{4.5}{63}{Results of the grid search shown for all loss functions, learning rates and batch sizes. Independent of the loss / learning rate combination a batch size of 32 performed consistently worse than a batch size of 64.\relax }{figure.caption.108}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Results of the YOLO grid search for all used loss functions and learning rates shown for batch size 64 on the validation dataset.\relax }}{63}{figure.caption.109}\protected@file@percent }
\newlabel{fig:yolo_grid_heat_results}{{4.6}{63}{Results of the YOLO grid search for all used loss functions and learning rates shown for batch size 64 on the validation dataset.\relax }{figure.caption.109}{}}
\citation{diou}
\citation{weighted_bbox_fusion}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Different input sizes tested after training on the validation set.\relax }}{64}{figure.caption.111}\protected@file@percent }
\newlabel{fig:yolo_input_size}{{4.7}{64}{Different input sizes tested after training on the validation set.\relax }{figure.caption.111}{}}
\@writefile{toc}{\contentsline {subsubsection}{Non-Maximum Suppression Tuning and Test-Time Augmentation }{64}{section*.112}\protected@file@percent }
\acronymused{NMS}
\acronymused{NMS}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{WBF}
\acronymused{TTA}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{WBF}
\acronymused{IoU}
\acronymused{WBF}
\citation{weighted_bbox_fusion}
\acronymused{FP}
\acronymused{WBF}
\acronymused{TTA}
\acronymused{WBF}
\acronymused{IoU}
\acronymused{WBF}
\acronymused{TTA}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{TTA}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{DIoU}
\acronymused{TTA}
\acronymused{WBF}
\@writefile{toc}{\contentsline {subsubsection}{Improve Model Uncertainty Through Voting Based Thresholding in \ac {WBF}}{65}{section*.113}\protected@file@percent }
\acronymused{TTA}
\acronymused{WBF}
\acronymused{FP}
\acronymused{WBF}
\acronymused{TTA}
\acronymused{WBF}
\acronymused{WBF}
\citation{mobile_unet}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Results of the voting based thresholding approach to improve model performance, shown on the validation set for all possible voting thresholds.\relax }}{66}{figure.caption.114}\protected@file@percent }
\newlabel{fig:wbf_tta_nms_votes}{{4.8}{66}{Results of the voting based thresholding approach to improve model performance, shown on the validation set for all possible voting thresholds.\relax }{figure.caption.114}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Results of the tuning experiment with the underlying threshold and input size combination, performed on the validation set, presented also with the performance on the test dataset. A visual representation can also be found in figure \ref  {fig:yolo_tuning_combined_results}.\relax }}{66}{table.4.3}\protected@file@percent }
\newlabel{tab:yolo_tuning_combined_results}{{4.3}{66}{Results of the tuning experiment with the underlying threshold and input size combination, performed on the validation set, presented also with the performance on the test dataset. A visual representation can also be found in figure \ref {fig:yolo_tuning_combined_results}.\relax }{table.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Final Results}{66}{subsection.4.1.5}\protected@file@percent }
\newlabel{sec:yolo_final}{{4.1.5}{66}{Final Results}{subsection.4.1.5}{}}
\acronymused{NMS}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{WBF}
\acronymused{WBF}
\acronymused{WBF}
\acronymused{NMS}
\acronymused{TTA}
\acronymused{TTA}
\acronymused{TTA}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces The final results of the tuning experiment performed on the validation set, presented also with the performance on the test dataset. Specific explanation of the experiments with the optimal obtained parameters can be found in table \ref  {tab:yolo_tuning_combined_results}.\relax }}{67}{figure.caption.115}\protected@file@percent }
\newlabel{fig:yolo_tuning_combined_results}{{4.9}{67}{The final results of the tuning experiment performed on the validation set, presented also with the performance on the test dataset. Specific explanation of the experiments with the optimal obtained parameters can be found in table \ref {tab:yolo_tuning_combined_results}.\relax }{figure.caption.115}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}MobileNetV2-UNet}{67}{section.4.2}\protected@file@percent }
\newlabel{sec:training_munet}{{4.2}{67}{MobileNetV2-UNet}{section.4.2}{}}
\acronymused{MUnet}
\acronymused{MUnet}
\acronymused{MUnet}
\acronymused{MUnet}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces An overview of the conducted experiments with the \ac {MUnet}. Best parameters which were obtained from a previous experiment were used in the following experiment.\relax }}{68}{figure.caption.116}\protected@file@percent }
\acronymused{MUnet}
\newlabel{fig:munet_experiments_overview}{{4.10}{68}{An overview of the conducted experiments with the \ac {MUnet}. Best parameters which were obtained from a previous experiment were used in the following experiment.\relax }{figure.caption.116}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Learning Rate Experiment}{68}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Augmentation Experiments}{68}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Offline Augmentations}{68}{section*.118}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Configuration of the YOLO grid search experiment. The networks were trained for all possible configurations of learning rate, batch size and loss function.\relax }}{68}{table.4.4}\protected@file@percent }
\newlabel{tab:munet_config}{{4.4}{68}{Configuration of the YOLO grid search experiment. The networks were trained for all possible configurations of learning rate, batch size and loss function.\relax }{table.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces The results of the initial learning rate search shown on the validation set, with the mean and standard deviation of the F1-Score over three separate training runs.\relax }}{69}{figure.caption.117}\protected@file@percent }
\newlabel{fig:munet_lr_exp}{{4.11}{69}{The results of the initial learning rate search shown on the validation set, with the mean and standard deviation of the F1-Score over three separate training runs.\relax }{figure.caption.117}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces The results of the offline augmentation experiment shown on the validation set, as the mean F1-Score of three separate training runs.\relax }}{69}{figure.caption.119}\protected@file@percent }
\newlabel{fig:munet_offline_exp}{{4.12}{69}{The results of the offline augmentation experiment shown on the validation set, as the mean F1-Score of three separate training runs.\relax }{figure.caption.119}{}}
\@writefile{toc}{\contentsline {subsubsection}{Online Augmentations}{69}{section*.120}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces The results of the online augmentation experiment shown on the validation set, as the mean F1-Score of three separate training runs.\relax }}{70}{figure.caption.121}\protected@file@percent }
\newlabel{fig:munet_online_exp}{{4.13}{70}{The results of the online augmentation experiment shown on the validation set, as the mean F1-Score of three separate training runs.\relax }{figure.caption.121}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Grid Search Experiment on Hyperparameters}{70}{subsection.4.2.3}\protected@file@percent }
\newlabel{sec:munet_grid}{{4.2.3}{70}{Grid Search Experiment on Hyperparameters}{subsection.4.2.3}{}}
\acronymused{MUnet}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Configuration of the \ac {MUnet} grid search experiment, with tested grid search parameters and fixed offline and offline augmentation parameters. The networks were trained for all possible combinations of learning rate, batch size and loss function.\relax }}{70}{table.4.5}\protected@file@percent }
\newlabel{tab:munet_grid_params}{{4.5}{70}{Configuration of the \ac {MUnet} grid search experiment, with tested grid search parameters and fixed offline and offline augmentation parameters. The networks were trained for all possible combinations of learning rate, batch size and loss function.\relax }{table.4.5}{}}
\acronymused{MUnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Final Results}{71}{subsection.4.2.4}\protected@file@percent }
\newlabel{sec:munet_final}{{4.2.4}{71}{Final Results}{subsection.4.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces The results of the selected \ac {MUnet} networks with a batch size of 32.\relax }}{73}{figure.caption.122}\protected@file@percent }
\acronymused{MUnet}
\newlabel{fig:munet_fold_32}{{4.14}{73}{The results of the selected \ac {MUnet} networks with a batch size of 32.\relax }{figure.caption.122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces The results of the selected \ac {MUnet} networks with a batch size of 64.\relax }}{74}{figure.caption.123}\protected@file@percent }
\acronymused{MUnet}
\newlabel{fig:munet_fold_64}{{4.15}{74}{The results of the selected \ac {MUnet} networks with a batch size of 64.\relax }{figure.caption.123}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Pipeline Evaluation}{75}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\acronymused{TP}
\acronymused{FP}
\acronymused{FN}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Evaluation Algorithm}{75}{section.5.1}\protected@file@percent }
\newlabel{sec:eval_algo}{{5.1}{75}{Evaluation Algorithm}{section.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Classification}{75}{subsection.5.1.1}\protected@file@percent }
\acronymused{YOLO}
\acronymused{mAP}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{TP}
\acronymused{FP}
\acronymused{FN}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Overview of the evaluation pipeline. Evaluation is split into classification evaluation of bounding boxes, topology evaluation and matching evaluation.\relax }}{76}{figure.caption.124}\protected@file@percent }
\newlabel{fig:eval_pipeline_overview}{{5.1}{76}{Overview of the evaluation pipeline. Evaluation is split into classification evaluation of bounding boxes, topology evaluation and matching evaluation.\relax }{figure.caption.124}{}}
\citation{graph_edit_distance}
\acronymused{MUnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Topology}{77}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The subedges in prediction and ground truth are the same}{77}{section*.125}\protected@file@percent }
\newlabel{eq:tph}{{5.1}{77}{The subedges in prediction and ground truth are the same}{equation.5.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{A subedge is missing}{77}{section*.126}\protected@file@percent }
\newlabel{eq:fns}{{5.2}{78}{A subedge is missing}{equation.5.1.2}{}}
\newlabel{eq:all_possible}{{5.3}{78}{A subedge is missing}{equation.5.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{A subedge too much}{78}{section*.127}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {5.1}{\ignorespaces Topology Evaluation Algorithm\relax }}{79}{algocf.5.1}\protected@file@percent }
\newlabel{alg:topology_eval}{{5.1}{79}{A subedge too much}{algocf.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Current Insufficiencies}{79}{section*.128}\protected@file@percent }
\acronymused{YOLO}
\citation{when_tta_works}
\citation{ecd_yolobool}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Annotation Matching}{80}{subsection.5.1.3}\protected@file@percent }
\acronymused{LDom}
\acronymused{ECC}
\acronymused{TP}
\acronymused{ECC}
\acronymused{FN}
\acronymused{ECC}
\acronymused{FP}
\acronymused{ECC}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Results}{80}{section.5.2}\protected@file@percent }
\newlabel{sec:evaluation_results}{{5.2}{80}{Results}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Classification}{80}{subsection.5.2.1}\protected@file@percent }
\acronymused{YOLO}
\acronymused{mAP}
\acronymused{IoU}
\acronymused{TP}
\acronymused{FP}
\acronymused{FN}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces The results of the YOLO classification for the tuned networks presented in section \ref  {sec:training_yolo}. Classification was done based on a matching \ac {IoU} threshold of 0.3.\relax }}{80}{table.5.1}\protected@file@percent }
\newlabel{tab:yolo_classification_res}{{5.1}{80}{The results of the YOLO classification for the tuned networks presented in section \ref {sec:training_yolo}. Classification was done based on a matching \ac {IoU} threshold of 0.3.\relax }{table.5.1}{}}
\acronymused{IoU}
\acronymused{TP}
\acronymused{FP}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{WBF}
\acronymused{TP}
\acronymused{FP}
\acronymused{TTA}
\acronymused{TP}
\acronymused{FP}
\acronymused{TP}
\acronymused{TTA}
\acronymused{TTA}
\acronymused{TTA}
\acronymused{TP}
\acronymused{FP}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Topology}{81}{subsection.5.2.2}\protected@file@percent }
\acronymused{YOLO}
\acronymused{MUnet}
\acronymused{YOLO}
\acronymused{YOLO}
\acronymused{YOLO}
\acronymused{MUnet}
\acronymused{FN}
\acronymused{FP}
\acronymused{FN}
\acronymused{TP}
\acronymused{FP}
\acronymused{TP}
\acronymused{TP}
\acronymused{TP}
\acronymused{TP}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Results of the topology evaluation. The focal loss folds all have $\gamma = 2$. The used networks were the ones selected in table \ref  {tab:munet_selected_nets}.\relax }}{83}{table.5.2}\protected@file@percent }
\newlabel{tab:topology_test_plain}{{5.2}{83}{Results of the topology evaluation. The focal loss folds all have $\gamma = 2$. The used networks were the ones selected in table \ref {tab:munet_selected_nets}.\relax }{table.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Results of the text matching, based on the \ac {YOLO} configurations from table \ref  {tab:yolo_classification_res}.\relax }}{84}{table.5.3}\protected@file@percent }
\newlabel{tab:text_matching_results}{{5.3}{84}{Results of the text matching, based on the \ac {YOLO} configurations from table \ref {tab:yolo_classification_res}.\relax }{table.5.3}{}}
\acronymused{YOLO}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Annotation Matching}{84}{subsection.5.2.3}\protected@file@percent }
\acronymused{ECC}
\acronymused{FN}
\acronymused{FP}
\acronymused{FP}
\acronymused{YOLO}
\acronymused{ECC}
\acronymused{FN}
\acronymused{YOLO}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Results of the arrow matching\relax }}{85}{table.5.4}\protected@file@percent }
\newlabel{tab:arrow_matching_results}{{5.4}{85}{Results of the arrow matching\relax }{table.5.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Summary and Outlook}{87}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Discussion}{87}{section.6.1}\protected@file@percent }
\acronymused{YOLO}
\@writefile{toc}{\contentsline {subsubsection}{\ac {YOLO}}{87}{section*.129}\protected@file@percent }
\acronymused{YOLO}
\acronymused{mAP}
\acronymused{mAP}
\acronymused{mAP}
\acronymused{MUnet}
\@writefile{toc}{\contentsline {subsubsection}{\ac {MUnet}}{87}{section*.130}\protected@file@percent }
\acronymused{MUnet}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{YOLO}
\@writefile{toc}{\contentsline {subsubsection}{Pipeline}{88}{section*.131}\protected@file@percent }
\acronymused{ECD}
\acronymused{YOLO}
\acronymused{MUnet}
\acronymused{TP}
\acronymused{FP}
\acronymused{FN}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation Algorithm}{88}{section*.132}\protected@file@percent }
\acronymused{YOLO}
\citation{mask_rcnn}
\citation{ecd_knnfull}
\citation{ecd_anngeo}
\citation{ecd_basecnn}
\citation{ecd_anngeo}
\citation{ecd_texturesmo}
\citation{ecd_knn_recog}
\citation{ecd_seghogsvm}
\citation{ecd_yolobool}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Future Work}{89}{section.6.2}\protected@file@percent }
\acronymused{ECD}
\acronymused{ECD}
\acronymused{OCR}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Summary}{89}{section.6.3}\protected@file@percent }
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECC}
\citation{lecun_lenet}
\citation{yolov1}
\citation{rcnn}
\citation{yolov4_tiny}
\citation{fpn}
\citation{diou}
\citation{weighted_bbox_fusion}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ANN}
\acronymused{CNN}
\acronymused{MLP}
\acronymused{ANN}
\acronymused{CNN}
\acronymused{YOLO}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{YOLO}
\citation{mask_rcnn}
\citation{semantic_segmentation}
\citation{mobile_unet}
\citation{mnetv2}
\citation{hypergraph_def}
\citation{hypergraph_adjacency}
\acronymused{YOLO}
\acronymused{YOLO}
\acronymused{NMS}
\acronymused{NMS}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{DIoU}
\acronymused{WBF}
\acronymused{IoU}
\acronymused{WBF}
\acronymused{TTA}
\acronymused{ECD}
\acronymused{MUnet}
\acronymused{YOLO}
\acronymused{MUnet}
\acronymused{MUnet}
\acronymused{CNN}
\acronymused{YOLO}
\acronymused{MUnet}
\acronymused{ECD}
\acronymused{ECC}
\acronymused{YOLO}
\acronymused{MUnet}
\acronymused{ECC}
\citation{copypaste_aug}
\acronymused{ECD}
\acronymused{OCR}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{YOLO}
\acronymused{MUnet}
\acronymused{ECD}
\acronymused{YOLO}
\acronymused{MUnet}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{OCR}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{YOLO}
\acronymused{MUnet}
\acronymused{YOLO}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{WBF}
\acronymused{WBF}
\acronymused{TTA}
\acronymused{YOLO}
\acronymused{mAP}
\acronymused{YOLO}
\acronymused{YOLO}
\acronymused{ECC}
\citation{graph_edit_distance}
\acronymused{MUnet}
\acronymused{YOLO}
\acronymused{IoU}
\acronymused{YOLO}
\acronymused{FP}
\acronymused{FN}
\acronymused{YOLO}
\acronymused{MUnet}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Conclusion}{93}{section.6.4}\protected@file@percent }
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{YOLOv4}
\acronymused{MUnet}
\acronymused{YOLO}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECD}
\newacro{ANN}[\AC@hyperlink{ANN}{ANN}]{Artificial Neural Network}
\newacro{AP}[\AC@hyperlink{AP}{AP}]{Average Precision}
\newacro{BatchNorm}[\AC@hyperlink{BatchNorm}{BatchNorm}]{Batch Normalization}
\newacro{CAD}[\AC@hyperlink{CAD}{CAD}]{Computer-Aided Design}
\newacro{CE}[\AC@hyperlink{CE}{CE}]{Cross Entropy}
\newacro{CIoU}[\AC@hyperlink{CIoU}{CIoU}]{Complete IoU}
\newacro{CNN}[\AC@hyperlink{CNN}{CNN}]{Convolutional Neural Network}
\newacro{CSV}[\AC@hyperlink{CSV}{CSV}]{Comma Separated Values}
\newacro{DIoU}[\AC@hyperlink{DIoU}{DIoU}]{Distance IoU}
\newacro{ECC}[\AC@hyperlink{ECC}{ECC}]{Electrical Circuit Component}
\newacro{ECD}[\AC@hyperlink{ECD}{ECD}]{Electrical Circuit Diagram}
\newacro{EIoU}[\AC@hyperlink{EIoU}{EIoU}]{Efficient IoU}
\newacro{FCOS}[\AC@hyperlink{FCOS}{FCOS}]{Fully Convolutional One Stage Detector}
\newacro{FC}[\AC@hyperlink{FC}{FC}]{Fully-Connected}
\newacro{FN}[\AC@hyperlink{FN}{FN}]{False Negative}
\newacro{FP}[\AC@hyperlink{FP}{FP}]{False Positive}
\newacro{GIoU}[\AC@hyperlink{GIoU}{GIoU}]{Generalized IoU}
\newacro{HOG}[\AC@hyperlink{HOG}{HOG}]{Histogram of Oriented Gradients}
\newacro{IDom}[\AC@hyperlink{IDom}{IDom}]{Image Domain}
\newacro{IoU}[\AC@hyperlink{IoU}{IoU}]{Intersection over Union}
\newacro{KNN}[\AC@hyperlink{KNN}{KNN}]{K-Nearest-Neighbor}
\newacro{LDom}[\AC@hyperlink{LDom}{LDom}]{LTspice Domain}
\newacro{LReLU}[\AC@hyperlink{LReLU}{LReLU}]{Leaky ReLU}
\newacro{MLP}[\AC@hyperlink{MLP}{MLP}]{Multilayer Perceptron}
\newacro{MSE}[\AC@hyperlink{MSE}{MSE}]{Mean Squared Error}
\newacro{MUnet}[\AC@hyperlink{MUnet}{MUnet}]{MobileNetV2-UNet}
\newacro{NMS}[\AC@hyperlink{NMS}{NMS}]{Non-Maximum Suppression}
\newacro{OCR}[\AC@hyperlink{OCR}{OCR}]{Optical Character Recognition}
\newacro{PANet}[\AC@hyperlink{PANet}{PANet}]{Path Aggregation Network}
\newacro{R-CNN}[\AC@hyperlink{R-CNN}{R-CNN}]{Regions with CNN features}
\newacro{ReLU}[\AC@hyperlink{ReLU}{ReLU}]{Rectified Linear Unit}
\newacro{RoI}[\AC@hyperlink{RoI}{RoI}]{Region of Interest}
\newacro{SGD}[\AC@hyperlink{SGD}{SGD}]{Stochastic Gradient Descent}
\newacro{SMO}[\AC@hyperlink{SMO}{SMO}]{Sequential Minimal Optimization}
\newacro{SSD}[\AC@hyperlink{SSD}{SSD}]{Single Shot Multibox Detector}
\newacro{SVM}[\AC@hyperlink{SVM}{SVM}]{Support Vector Machine}
\newacro{TN}[\AC@hyperlink{TN}{TN}]{True Negative}
\newacro{TP}[\AC@hyperlink{TP}{TP}]{True Positive}
\newacro{TTA}[\AC@hyperlink{TTA}{TTA}]{Test-Time Augmentation}
\newacro{WBF}[\AC@hyperlink{WBF}{WBF}]{Weighted Bounding Box Fusion}
\newacro{YOLOv1}[\AC@hyperlink{YOLOv1}{YOLOv1}]{You Only Look Once Version 1}
\newacro{YOLOv4}[\AC@hyperlink{YOLOv4}{YOLOv4}]{You Only Look Once Version 4}
\newacro{YOLO}[\AC@hyperlink{YOLO}{YOLO}]{You Only Look Once}
\newacro{mAP}[\AC@hyperlink{mAP}{mAP}]{Mean Average Precision}
\newacro{mIoU}[\AC@hyperlink{mIoU}{mIoU}]{Mean Intersection over Union}
\@input{mt-lof.aux}
\@input{mt-lot.aux}
\@input{mt-lit.aux}
\@writefile{toc}{\contentsline {chapter}{Appendix}{117}{appendix*.137}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}YOLOv4-Tiny Experiments}{117}{section.Alph0.1}\protected@file@percent }
\newlabel{app:yolo_experiments}{{A}{117}{YOLOv4-Tiny Experiments}{section.Alph0.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces The results of the initial learning rate search shown on the validation set, with mAP over all classes as well as the classwise AP. The mean and standard deviation are taken over three separate runs.\relax }}{118}{table.Alph0.1.1}\protected@file@percent }
\newlabel{tab:yolo_init_lr_results}{{A.1}{118}{The results of the initial learning rate search shown on the validation set, with mAP over all classes as well as the classwise AP. The mean and standard deviation are taken over three separate runs.\relax }{table.Alph0.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces The results of the offline augmentation configuration search. The letter ``P'' stands for projection, ``F'' for flip and ``R'' means rotation. Rotation always includes a 90\textdegree \, 180\textdegree \ and a 270\textdegree \ rotation. Flip is a horizontal flip. All results are given with the mean and standard deviation over three runs and the results are compared against the baseline, which is the best performing learning rate from the learning rate search experiment (table \ref  {tab:yolo_init_lr_results}).\relax }}{119}{table.Alph0.1.2}\protected@file@percent }
\newlabel{tab:yolo_offline_aug_results}{{A.2}{119}{The results of the offline augmentation configuration search. The letter ``P'' stands for projection, ``F'' for flip and ``R'' means rotation. Rotation always includes a 90\textdegree \, 180\textdegree \ and a 270\textdegree \ rotation. Flip is a horizontal flip. All results are given with the mean and standard deviation over three runs and the results are compared against the baseline, which is the best performing learning rate from the learning rate search experiment (table \ref {tab:yolo_init_lr_results}).\relax }{table.Alph0.1.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces The results of the rotation augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{120}{table.Alph0.1.3}\protected@file@percent }
\newlabel{tab:yolo_rotation_augmentation_result}{{A.3}{120}{The results of the rotation augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }{table.Alph0.1.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces The results of the random scale augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{121}{table.Alph0.1.4}\protected@file@percent }
\newlabel{tab:yolo_random_scale_augmentation_result}{{A.4}{121}{The results of the random scale augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }{table.Alph0.1.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.5}{\ignorespaces The results of the safe bounding box crop augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{122}{table.Alph0.1.5}\protected@file@percent }
\newlabel{tab:yolo_bbox_safe_crop_augmentation_result}{{A.5}{122}{The results of the safe bounding box crop augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }{table.Alph0.1.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.6}{\ignorespaces The results of the color jitter augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{123}{table.Alph0.1.6}\protected@file@percent }
\newlabel{tab:yolo_color_jitter_augmentation_result}{{A.6}{123}{The results of the color jitter augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }{table.Alph0.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces The results of the \ac {DIoU}-\ac {NMS} parameter tuning. All possible combinations of score threshold and \ac {IoU} threshold were evaluated on the validation set. The best performing combination is the one with a score threshold of 0.1 and a \ac {IoU} threshold of 0.45.\relax }}{124}{figure.caption.138}\protected@file@percent }
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{IoU}
\acronymused{IoU}
\newlabel{fig:diou_nms_tuning}{{A.1}{124}{The results of the \ac {DIoU}-\ac {NMS} parameter tuning. All possible combinations of score threshold and \ac {IoU} threshold were evaluated on the validation set. The best performing combination is the one with a score threshold of 0.1 and a \ac {IoU} threshold of 0.45.\relax }{figure.caption.138}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces The results of the \ac {WBF} parameter tuning. All possible combinations of score threshold and \ac {IoU} threshold were evaluated on the validation set. The best performing combination is the one with a score threshold of 0.15 and an \ac {IoU} threshold of 0.25.\relax }}{125}{figure.caption.139}\protected@file@percent }
\acronymused{WBF}
\acronymused{IoU}
\acronymused{IoU}
\newlabel{fig:wbf_nms_tuning}{{A.2}{125}{The results of the \ac {WBF} parameter tuning. All possible combinations of score threshold and \ac {IoU} threshold were evaluated on the validation set. The best performing combination is the one with a score threshold of 0.15 and an \ac {IoU} threshold of 0.25.\relax }{figure.caption.139}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces The results of the \ac {WBF} tuning with \ac {TTA}. All possible combinations of score threshold and \ac {IoU} threshold were evaluated on the validation set. The best performing combination is the one with a score threshold of 0.1 and an \ac {IoU} threshold of 0.45.\relax }}{126}{figure.caption.140}\protected@file@percent }
\acronymused{WBF}
\acronymused{TTA}
\acronymused{IoU}
\acronymused{IoU}
\newlabel{fig:wbf_tta_nms_tuning}{{A.3}{126}{The results of the \ac {WBF} tuning with \ac {TTA}. All possible combinations of score threshold and \ac {IoU} threshold were evaluated on the validation set. The best performing combination is the one with a score threshold of 0.1 and an \ac {IoU} threshold of 0.45.\relax }{figure.caption.140}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}MobileNetV2-UNet Experiments}{127}{section.Alph0.2}\protected@file@percent }
\newlabel{app:munet_experiments}{{B}{127}{MobileNetV2-UNet Experiments}{section.Alph0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces F1-Score results of the \ac {MUnet} grid search experiment shown on the full validation data. Results are shown combined for the loss and batch size against the learning rate.\relax }}{127}{figure.caption.141}\protected@file@percent }
\acronymused{MUnet}
\newlabel{fig:munet_f1b_heat}{{B.1}{127}{F1-Score results of the \ac {MUnet} grid search experiment shown on the full validation data. Results are shown combined for the loss and batch size against the learning rate.\relax }{figure.caption.141}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Recall results of the \ac {MUnet} grid search experiment shown on the full validation data. Results are shown combined for the loss and batch size against the learning rate.\relax }}{128}{figure.caption.142}\protected@file@percent }
\acronymused{MUnet}
\newlabel{fig:munet_rb_heat}{{B.2}{128}{Recall results of the \ac {MUnet} grid search experiment shown on the full validation data. Results are shown combined for the loss and batch size against the learning rate.\relax }{figure.caption.142}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Precision results of the \ac {MUnet} grid search experiment shown on the full validation data. Results are shown combined for the loss and batch size against the learning rate.\relax }}{129}{figure.caption.143}\protected@file@percent }
\acronymused{MUnet}
\newlabel{fig:munet_pb_heat}{{B.3}{129}{Precision results of the \ac {MUnet} grid search experiment shown on the full validation data. Results are shown combined for the loss and batch size against the learning rate.\relax }{figure.caption.143}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces F1-Score results of the \ac {MUnet} grid search experiment shown validation data, for checkered images only. Results are shown combined for the loss and batch size against the learning rate.\relax }}{130}{figure.caption.144}\protected@file@percent }
\acronymused{MUnet}
\newlabel{fig:munet_f1c_heat}{{B.4}{130}{F1-Score results of the \ac {MUnet} grid search experiment shown validation data, for checkered images only. Results are shown combined for the loss and batch size against the learning rate.\relax }{figure.caption.144}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces Recall results of the \ac {MUnet} grid search experiment shown validation data, for checkered images only. Results are shown combined for the loss and batch size against the learning rate.\relax }}{131}{figure.caption.145}\protected@file@percent }
\acronymused{MUnet}
\newlabel{fig:munet_rc_heat}{{B.5}{131}{Recall results of the \ac {MUnet} grid search experiment shown validation data, for checkered images only. Results are shown combined for the loss and batch size against the learning rate.\relax }{figure.caption.145}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.6}{\ignorespaces Precision results of the \ac {MUnet} grid search experiment shown validation data, for checkered images only. Results are shown combined for the loss and batch size against the learning rate.\relax }}{132}{figure.caption.146}\protected@file@percent }
\acronymused{MUnet}
\newlabel{fig:munet_pc_heat}{{B.6}{132}{Precision results of the \ac {MUnet} grid search experiment shown validation data, for checkered images only. Results are shown combined for the loss and batch size against the learning rate.\relax }{figure.caption.146}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces The selected \ac {MUnet} folds (combination of batch size and loss) from the grid search experiment. From each fold the best performing metrics were selected, based on the full validation set and the validation set with checkered backgrounds only. The metric values indicate the mean score of that particular fold. From each fold then the best performing training run has been selected as a network for further testing. Overall 36 folds have been selected. Some combinations appeared more then once, hence overall 26 networks have been in the end selected.\relax }}{133}{table.Alph0.2.1}\protected@file@percent }
\newlabel{tab:munet_selected_nets}{{B.1}{133}{The selected \ac {MUnet} folds (combination of batch size and loss) from the grid search experiment. From each fold the best performing metrics were selected, based on the full validation set and the validation set with checkered backgrounds only. The metric values indicate the mean score of that particular fold. From each fold then the best performing training run has been selected as a network for further testing. Overall 36 folds have been selected. Some combinations appeared more then once, hence overall 26 networks have been in the end selected.\relax }{table.Alph0.2.1}{}}
\acronymused{MUnet}
\gdef \@abspage@last{143}

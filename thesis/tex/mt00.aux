\relax 
\AC@reset@newl@bel
\bibstyle{alphamod}
\citation{ecd_basecnn}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Related Works}{1}{}\protected@file@percent }
\AC@undonewlabel{acro:ECD}
\newlabel{acro:ECD}{{1.2}{1}}
\acronymused{ECD}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Classification of Eletrical Circuit Components}{1}{}\protected@file@percent }
\AC@undonewlabel{acro:CNN}
\newlabel{acro:CNN}{{1.2.1}{1}}
\acronymused{CNN}
\AC@undonewlabel{acro:ECC}
\newlabel{acro:ECC}{{1.2.1}{1}}
\acronymused{ECC}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Goals of the Thesis}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Task Description}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Contribution}{1}{}\protected@file@percent }
\citation{iec60617}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theory}{3}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Electrical Circuit Diagrams}{3}{}\protected@file@percent }
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECD}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces All used \acp {ECC} in this thesis in German notation: 1. Voltage Source, 2. Current Source, 3. Resistor, 4. Inductor, 5. Capacitor, 6. Diode, 7. Ground\relax }}{3}{}\protected@file@percent }
\acronymused{ECC}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:used_eccs}{{2.1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}LTspice}{4}{}\protected@file@percent }
\acronymused{ECD}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}LTspice Schematic File Syntax}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Header}{4}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces LTspice header syntax\relax }}{4}{}\protected@file@percent }
\newlabel{tab:ltheader_syntax}{{2.1}{4}}
\@writefile{toc}{\contentsline {subsubsection}{Symbols}{4}{}\protected@file@percent }
\acronymused{ECC}
\acronymused{ECC}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces LTspice symbol names\relax }}{5}{}\protected@file@percent }
\newlabel{tab:ltsymbol_mapping}{{2.2}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces LTspice symbol syntax\relax }}{5}{}\protected@file@percent }
\newlabel{tab:ltsymbol_syntax}{{2.3}{5}}
\@writefile{toc}{\contentsline {subsubsection}{Symbol Attributes}{5}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces LTspice symbol attribute syntax\relax }}{5}{}\protected@file@percent }
\newlabel{tab:ltsymattr_syntax}{{2.4}{5}}
\citation{bioneuron}
\citation{perceptron}
\@writefile{toc}{\contentsline {subsubsection}{Ground}{6}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces LTspice ground syntax\relax }}{6}{}\protected@file@percent }
\newlabel{tab:ltflag_syntax}{{2.5}{6}}
\@writefile{toc}{\contentsline {subsubsection}{Wire}{6}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces LTspice wire syntax\relax }}{6}{}\protected@file@percent }
\newlabel{tab:ltwire_syntax}{{2.6}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Artificial Neural Networks}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}General Concepts}{6}{}\protected@file@percent }
\newlabel{sec:deep_basics}{{2.3.1}{6}}
\AC@undonewlabel{acro:ANN}
\newlabel{acro:ANN}{{2.3.1}{6}}
\acronymused{ANN}
\citation{mlp}
\citation{dl}
\citation{dl_mit}
\citation{dl}
\citation{dl_mit}
\citation{softmax}
\newlabel{form:perceptron}{{2.1}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Multi Layer Perceptron}{7}{}\protected@file@percent }
\AC@undonewlabel{acro:MLP}
\newlabel{acro:MLP}{{2.3.1}{7}}
\acronymused{MLP}
\acronymused{MLP}
\newlabel{eq:identity}{{2.2}{7}}
\AC@undonewlabel{acro:FC}
\newlabel{acro:FC}{{2.3.1}{7}}
\acronymused{FC}
\newlabel{eq:fc_weights}{{2.4}{7}}
\citation{yolov1}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A MLP with two hidden layers (two neurons in the first and one in the last), each input gets multiplied with each weight of each neuron, summed up and fed into an activation function to produce the input for the next layer, where this process is repeated.\relax }}{8}{}\protected@file@percent }
\newlabel{fig:mlp}{{2.2}{8}}
\acronymused{MLP}
\@writefile{toc}{\contentsline {subsubsection}{Learning Procedure}{8}{}\protected@file@percent }
\citation{loss_function_segmentation}
\citation{sgd_momentum}
\citation{adam}
\citation{dl}
\citation{mish}
\AC@undonewlabel{acro:MSE}
\newlabel{acro:MSE}{{2.3.1}{9}}
\acronymused{MSE}
\newlabel{eq:mse}{{2.6}{9}}
\AC@undonewlabel{acro:CE}
\newlabel{acro:CE}{{2.3.1}{9}}
\acronymused{CE}
\newlabel{eq:ce}{{2.7}{9}}
\acronymused{MLP}
\citation{dl}
\citation{lecun_lenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Activation Functions}{10}{}\protected@file@percent }
\newlabel{sec:activation_functions}{{2.3.2}{10}}
\acronymused{ANN}
\acronymused{ANN}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid}{10}{}\protected@file@percent }
\acronymused{ANN}
\acronymused{ANN}
\newlabel{eq:sigmoid}{{2.11}{10}}
\@writefile{toc}{\contentsline {subsubsection}{Rectified Linear Unit (ReLU)}{10}{}\protected@file@percent }
\AC@undonewlabel{acro:ReLU}
\newlabel{acro:ReLU}{{2.3.2}{10}}
\acronymused{ReLU}
\acronymused{ReLU}
\acronymused{ReLU}
\AC@undonewlabel{acro:LReLU}
\newlabel{acro:LReLU}{{2.3.2}{10}}
\acronymused{LReLU}
\acronymused{LReLU}
\acronymused{ReLU}
\citation{inception}
\citation{resnet}
\citation{densenet}
\citation{dl}
\citation{conv_arithmetic}
\citation{conv_arithmetic}
\citation{dl}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Convolutional Neural Networks}{11}{}\protected@file@percent }
\acronymused{MLP}
\acronymused{ANN}
\acronymused{CNN}
\acronymused{CNN}
\@writefile{toc}{\contentsline {subsubsection}{Convolutional Layer}{11}{}\protected@file@percent }
\acronymused{CNN}
\newlabel{eq:conv}{{2.14}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Pooling Layer}{11}{}\protected@file@percent }
\newlabel{sec:max_pooling}{{2.3.3}{11}}
\acronymused{CNN}
\citation{batchnorm}
\citation{augmentation_survey}
\citation{copypaste_aug}
\citation{albumentation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example convolution of a 4x4 input (blue) with a 3x3 kernel (dark blue) and a stride of 1, resulting in a 2x2 output (cyan) \cite  {conv_arithmetic}\relax }}{12}{}\protected@file@percent }
\newlabel{fig:conv_example}{{2.3}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Fully-Connected Layer}{12}{}\protected@file@percent }
\acronymused{FC}
\acronymused{CNN}
\acronymused{FC}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Batch Normalization}{12}{}\protected@file@percent }
\acronymused{ReLU}
\acronymused{ReLU}
\AC@undonewlabel{acro:BatchNorm}
\newlabel{acro:BatchNorm}{{2.3.4}{12}}
\acronymused{BatchNorm}
\acronymused{ANN}
\acronymused{BatchNorm}
\citation{albumentation}
\citation{albumentation}
\citation{tta_segmentation_cells}
\citation{when_tta_works}
\citation{when_tta_works}
\citation{yolov1}
\@writefile{lot}{\contentsline {table}{\numberline {2.7}{\ignorespaces A listing of the different augmentations used from albumentations \cite  {albumentation} in this thesis and the target domain where they were applied to.\relax }}{13}{}\protected@file@percent }
\newlabel{tab:used_augmentations}{{2.7}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Data Augmentation}{13}{}\protected@file@percent }
\acronymused{CNN}
\acronymused{ECD}
\AC@undonewlabel{acro:TTA}
\newlabel{acro:TTA}{{2.4}{13}}
\acronymused{TTA}
\acronymused{TTA}
\citation{sliding_window_satelite}
\citation{rcnn}
\citation{selective_search}
\citation{bbox_regressor}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Object Detection}{14}{}\protected@file@percent }
\newlabel{sec:object_detection}{{2.5}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}History of Object Detection}{14}{}\protected@file@percent }
\newlabel{sec:hostory_obj_detection}{{2.5.1}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Sliding Window}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Regions with CNN Features (R-CNN)}{14}{}\protected@file@percent }
\AC@undonewlabel{acro:R-CNN}
\newlabel{acro:R-CNN}{{2.5.1}{14}}
\acronymused{R-CNN}
\citation{selective_search}
\citation{selective_search}
\citation{fast_rcnn}
\citation{rcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Example of results obtained through the Selective Search algorithm (top) with increasing region scale from left to right and bounding boxes drawn around those regions (bottom). Selective Search produces sub-segmentations of objects in an image, considering size, color, texture and shape based features for the grouping of the regions. \cite  {selective_search}\relax }}{15}{}\protected@file@percent }
\newlabel{fig:selective_search}{{2.4}{15}}
\acronymused{CNN}
\acronymused{CNN}
\AC@undonewlabel{acro:SVM}
\newlabel{acro:SVM}{{2.5.1}{15}}
\acronymused{SVM}
\acronymused{SVM}
\@writefile{toc}{\contentsline {subsubsection}{Fast R-CNN}{15}{}\protected@file@percent }
\acronymused{R-CNN}
\acronymused{R-CNN}
\acronymused{R-CNN}
\acronymused{CNN}
\acronymused{CNN}
\AC@undonewlabel{acro:RoI}
\newlabel{acro:RoI}{{2.5.1}{15}}
\acronymused{RoI}
\acronymused{CNN}
\acronymused{RoI}
\citation{faster_rcnn}
\citation{yolov1}
\citation{ssd}
\citation{focalloss}
\citation{yolov3}
\citation{faster_rcnn}
\citation{yolov2}
\citation{yolov1}
\citation{corner_net}
\citation{center_net}
\citation{fcos}
\citation{fcos}
\citation{center_net}
\citation{yolov4}
\citation{giou}
\citation{iou}
\acronymused{RoI}
\acronymused{SVM}
\acronymused{R-CNN}
\@writefile{toc}{\contentsline {subsubsection}{Anchor Box Based Single Shot Detectors}{16}{}\protected@file@percent }
\acronymused{R-CNN}
\AC@undonewlabel{acro:YOLO}
\newlabel{acro:YOLO}{{2.5.1}{16}}
\acronymused{YOLO}
\AC@undonewlabel{acro:SSD}
\newlabel{acro:SSD}{{2.5.1}{16}}
\acronymused{SSD}
\@writefile{toc}{\contentsline {subsubsection}{Anchor Box Free Single Shot Detectors}{16}{}\protected@file@percent }
\AC@undonewlabel{acro:YOLOv1}
\newlabel{acro:YOLOv1}{{2.5.1}{16}}
\acronymused{YOLOv1}
\AC@undonewlabel{acro:FCOS}
\newlabel{acro:FCOS}{{2.5.1}{16}}
\acronymused{FCOS}
\AC@undonewlabel{acro:IoU}
\newlabel{acro:IoU}{{2.5.1}{16}}
\acronymused{IoU}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Intersection Over Union (IoU)}{16}{}\protected@file@percent }
\acronymused{IoU}
\acronymused{IoU}
\citation{iou}
\citation{giou}
\citation{eiou}
\citation{diou}
\citation{eiou}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}IoU Based Loss Functions}{17}{}\protected@file@percent }
\acronymused{MSE}
\acronymused{IoU}
\acronymused{IoU}
\newlabel{eq:iou_loss}{{2.17}{17}}
\AC@undonewlabel{acro:GIoU}
\newlabel{acro:GIoU}{{2.5.3}{17}}
\acronymused{GIoU}
\acronymused{IoU}
\acronymused{GIoU}
\acronymused{GIoU}
\acronymused{IoU}
\acronymused{GIoU}
\newlabel{eq:giou_loss}{{2.18}{17}}
\acronymused{IoU}
\AC@undonewlabel{acro:DIoU}
\newlabel{acro:DIoU}{{2.5.3}{17}}
\acronymused{DIoU}
\AC@undonewlabel{acro:CIoU}
\newlabel{acro:CIoU}{{2.5.3}{17}}
\acronymused{CIoU}
\acronymused{GIoU}
\acronymused{DIoU}
\newlabel{eq:diou_loss}{{2.19}{17}}
\acronymused{DIoU}
\acronymused{CIoU}
\acronymused{CIoU}
\citation{eiou}
\citation{yolov1}
\citation{yolov4}
\citation{yolov4_tiny}
\newlabel{eq:ciou_loss}{{2.20}{18}}
\newlabel{eq:ciou_nu}{{2.21}{18}}
\newlabel{eq:ciou_alpha}{{2.22}{18}}
\AC@undonewlabel{acro:EIoU}
\newlabel{acro:EIoU}{{2.5.3}{18}}
\acronymused{EIoU}
\newlabel{eq:eiou_loss}{{2.23}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}You Only Look Once (YOLO)}{18}{}\protected@file@percent }
\acronymused{YOLO}
\acronymused{YOLO}
\acronymused{YOLO}
\AC@undonewlabel{acro:YOLOv4}
\newlabel{acro:YOLOv4}{{2.6}{18}}
\acronymused{YOLOv4}
\acronymused{YOLOv4}
\@writefile{toc}{\contentsline {subsubsection}{General}{18}{}\protected@file@percent }
\acronymused{YOLOv4}
\citation{batchnorm}
\citation{yolov4}
\citation{yolov4}
\citation{pannet}
\citation{yolov4_tiny}
\@writefile{toc}{\contentsline {subsubsection}{Backbone}{19}{}\protected@file@percent }
\acronymused{YOLOv4}
\acronymused{ReLU}
\@writefile{toc}{\contentsline {subsubsection}{Neck}{19}{}\protected@file@percent }
\acronymused{YOLOv4}
\AC@undonewlabel{acro:PANet}
\newlabel{acro:PANet}{{2.6}{19}}
\acronymused{PANet}
\@writefile{lot}{\contentsline {table}{\numberline {2.8}{\ignorespaces The CSPDarknet53Tiny Architecture is a scaled version of the original CSPDarknet53 architecture \cite  {yolov4}. The definition for YOLOConv can be found in table 2.9\hbox {}. MaxPool indicates here the Max Pooling operation as described in sec. 2.3.3\hbox {}. The network is build out of five blocks, the first block reducing the image size and the following blocks building up features. Further, features from three different scales are taken and used as skip connections to the following network.\relax }}{20}{}\protected@file@percent }
\newlabel{tab:darknet_tiny_arch}{{2.8}{20}}
\@writefile{lot}{\contentsline {table}{\numberline {2.9}{\ignorespaces Convolutional basic building block in YOLOv4 (YOLOConv). A convolutional layer, followed by a batch normalization layer, followed by a leaky ReLU activation function.\relax }}{20}{}\protected@file@percent }
\newlabel{tab:yoloconv}{{2.9}{20}}
\citation{pannet}
\citation{pannet}
\citation{yolov3}
\@writefile{lot}{\contentsline {table}{\numberline {2.10}{\ignorespaces The PANetTiny architecture which is a scaled version of PANet \cite  {pannet}, which is the decoder in the YOLO network. It takes as inputs the skip connections from the CSPDarknet53Tiny. The network is build by alternating an output branch and an upsampling branch. First, the most lowest skip connection $Skip_L$ from the backbone is convolved and the output fed into the first output layer. Each output layer has again a $3\times 3$ convolution followed by a $1\times 1$ convolution, which form a raw bounding box prediction, fed to the YOLO head. After an output has been processed the previous convolution is again convolved and upsampled to be fed into the next output block. This is done three times in the whole PANetTiny network.\relax }}{21}{}\protected@file@percent }
\newlabel{tab:panet_tiny_arch}{{2.10}{21}}
\@writefile{toc}{\contentsline {subsubsection}{Head}{21}{}\protected@file@percent }
\acronymused{YOLOv4}
\citation{yolov2}
\citation{fast_rcnn}
\citation{faster_rcnn}
\citation{yolov2}
\citation{fast_rcnn}
\citation{faster_rcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Bounding box calculation in the YOLOv4 network based on a prior anchor box \cite  {yolov2}. The $b_*$ indicate the final prediction, $p_*$ indicate parameters of the prior bounding box and $c_*$ indicate the prior bounding box spatial offset based on the current cell. The final center coordinate offset is predicted non-linearly with a sigmoid function, while the final width and height are predicted linearly, as it is done in \cite  {fast_rcnn} and \cite  {faster_rcnn}.\relax }}{22}{}\protected@file@percent }
\newlabel{fig:bbox_calculation}{{2.5}{22}}
\@writefile{toc}{\contentsline {subsubsection}{Loss}{22}{}\protected@file@percent }
\acronymused{YOLO}
\acronymused{IoU}
\acronymused{YOLOv4}
\acronymused{CE}
\newlabel{eq:yolo_lclass}{{2.25}{23}}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{GIoU}
\acronymused{DIoU}
\acronymused{CIoU}
\acronymused{EIoU}
\newlabel{eq:yolo_lbbox}{{2.26}{23}}
\acronymused{CE}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{IoU}
\newlabel{eq:yolo_lobj}{{2.28}{23}}
\citation{diou}
\citation{soft_nms}
\newlabel{eq:yolo_lnoobj}{{2.29}{24}}
\acronymused{YOLO}
\newlabel{eq:yolo_loss}{{2.30}{24}}
\@writefile{toc}{\contentsline {subsubsection}{Non-Maximum Suppression (NMS)}{24}{}\protected@file@percent }
\AC@undonewlabel{acro:NMS}
\newlabel{acro:NMS}{{2.6}{24}}
\acronymused{NMS}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{NMS}
\acronymused{DIoU}
\acronymused{NMS}
\acronymused{DIoU}
\newlabel{alg1}{{2.1}{24}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.1}DIoU-NMS Algorithm TODO caption to bottom and format}{24}{}\protected@file@percent }
\citation{weighted_bbox_fusion}
\AC@undonewlabel{acro:WBF}
\newlabel{acro:WBF}{{2.6}{25}}
\acronymused{WBF}
\acronymused{IoU}
\acronymused{TTA}
\newlabel{eq:confidence_rescale}{{2.34}{26}}
\newlabel{eq:confidence_rescale_not_used}{{2.35}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Segmentation}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Hypergraphs}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Metrics}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{True Positive, False Positive, False Negative}{26}{}\protected@file@percent }
\AC@undonewlabel{acro:TP}
\newlabel{acro:TP}{{2.9}{26}}
\acronymused{TP}
\AC@undonewlabel{acro:FP}
\newlabel{acro:FP}{{2.9}{26}}
\acronymused{FP}
\AC@undonewlabel{acro:FN}
\newlabel{acro:FN}{{2.9}{26}}
\acronymused{FN}
\acronymused{TP}
\acronymused{FP}
\acronymused{FN}
\citation{map_pascal_voc}
\citation{map_coco}
\acronymused{TP}
\@writefile{toc}{\contentsline {subsubsection}{Precision}{27}{}\protected@file@percent }
\acronymused{TP}
\newlabel{eq:precision}{{2.36}{27}}
\@writefile{toc}{\contentsline {subsubsection}{Recall}{27}{}\protected@file@percent }
\newlabel{eq:recall}{{2.37}{27}}
\@writefile{toc}{\contentsline {subsubsection}{F1-Score}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Average Precision (AP)}{27}{}\protected@file@percent }
\AC@undonewlabel{acro:AP}
\newlabel{acro:AP}{{2.9}{27}}
\acronymused{AP}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{AP}
\citation{map_article}
\citation{map_article}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Example of a precision-recall curve, where precision and recall were calculated for different \ac {IoU} thresholds and sorted and plotted by their recall values \cite  {map_article}. The \ac {AP} is the area under the curve.\relax }}{28}{}\protected@file@percent }
\acronymused{IoU}
\acronymused{AP}
\newlabel{fig:pr_curve}{{2.6}{28}}
\newlabel{eq:ap}{{2.39}{28}}
\@writefile{toc}{\contentsline {subsubsection}{Mean Average Precision (mAP)}{28}{}\protected@file@percent }
\acronymused{AP}
\AC@undonewlabel{acro:mAP}
\newlabel{acro:mAP}{{2.9}{28}}
\acronymused{mAP}
\acronymused{AP}
\newlabel{eq:map}{{2.40}{28}}
\@writefile{toc}{\contentsline {subsubsection}{Mean Intersection over Union (mIoU)}{28}{}\protected@file@percent }
\AC@undonewlabel{acro:mIoU}
\newlabel{acro:mIoU}{{2.9}{28}}
\acronymused{mIoU}
\acronymused{IoU}
\acronymused{IoU}
\acronymused{mIoU}
\newlabel{eq:miou}{{2.41}{28}}
\citation{labelme}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Material and Methods}{29}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Data}{29}{}\protected@file@percent }
\acronymused{ECD}
\acronymused{ECC}
\acronymused{ECC}
\acronymused{ECD}
\acronymused{ECD}
\@writefile{toc}{\contentsline {subsubsection}{Labeling}{29}{}\protected@file@percent }
\acronymused{YOLOv4}
\acronymused{YOLOv4}
\acronymused{ECC}
\citation{canny_edge}
\citation{opencv}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Amount of images of \acp {ECD} used in this thesis shown with their underlying background and whether they are annotated or not. Further, the train / validation / test split of the different image types is shown. While this might seem like a big split for test, the number of bounding boxes included in the test set is way smaller and is shown in table 3.2\hbox {}\relax }}{30}{}\protected@file@percent }
\newlabel{tab:data_distribution}{{3.1}{30}}
\acronymused{ECD}
\acronymused{ECD}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Recognition and Conversion Pipeline}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}General Overview}{30}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces The classes present in this thesis with their major class which is an \ac {ECC} and alternatively their orientation. Furthermore, the total amount of classes is shown and the train, valid, test ratio.\relax }}{31}{}\protected@file@percent }
\newlabel{tab:yolo_classes}{{3.2}{31}}
\acronymused{ECC}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Training}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}YOLOv4-Tiny}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV2-UNet}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{35}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{37}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@input{mt07.aux}
\@input{mt08.aux}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Abbrevations}{39}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newacro{ANN}[\AC@hyperlink{ANN}{ANN}]{Artificial Neural Network}
\newacro{CCA}[\AC@hyperlink{CCA}{CCA}]{Connected Component Analysis}
\newacro{CNN}[\AC@hyperlink{CNN}{CNN}]{Convolutional Neural Network}
\newacro{ECC}[\AC@hyperlink{ECC}{ECC}]{Electrical Circuit Component}
\newacro{ECD}[\AC@hyperlink{ECD}{ECD}]{Electrical Circuit Diagram}
\newacro{MLP}[\AC@hyperlink{MLP}{MLP}]{Multi Layer Perceptron}
\newacro{MSE}[\AC@hyperlink{MSE}{MSE}]{Mean Squared Error}
\newacro{NMS}[\AC@hyperlink{NMS}{NMS}]{Non-Maximum Suppression}
\newacro{OCR}[\AC@hyperlink{OCR}{OCR}]{Optical Character Recognition}
\newacro{R-CNN}[\AC@hyperlink{R-CNN}{R-CNN}]{Regions with CNN features}
\newacro{YOLO}[\AC@hyperlink{YOLO}{YOLO}]{You Only Look Once}
\newacro{YOLOv1}[\AC@hyperlink{YOLOv1}{YOLOv1}]{You Only Look Once Version 1}
\newacro{YOLOv4}[\AC@hyperlink{YOLOv4}{YOLOv4}]{You Only Look Once Version 4}
\newacro{SVM}[\AC@hyperlink{SVM}{SVM}]{Support Vector Machine}
\newacro{RoI}[\AC@hyperlink{RoI}{RoI}]{Region of Interest}
\newacro{RPN}[\AC@hyperlink{RPN}{RPN}]{Region Proposal Network}
\newacro{IoU}[\AC@hyperlink{IoU}{IoU}]{Intersection over Union}
\newacro{GIoU}[\AC@hyperlink{GIoU}{GIoU}]{Generalized IoU}
\newacro{DIoU}[\AC@hyperlink{DIoU}{DIoU}]{Distance IoU}
\newacro{CIoU}[\AC@hyperlink{CIoU}{CIoU}]{Complete IoU}
\newacro{EIoU}[\AC@hyperlink{EIoU}{EIoU}]{Efficient IoU}
\newacro{CE}[\AC@hyperlink{CE}{CE}]{Cross Entropy}
\newacro{SSD}[\AC@hyperlink{SSD}{SSD}]{Single Shot Multibox Detector}
\newacro{ReLU}[\AC@hyperlink{ReLU}{ReLU}]{Rectified Linear Unit}
\newacro{LReLU}[\AC@hyperlink{LReLU}{LReLU}]{Leaky ReLU}
\newacro{PANet}[\AC@hyperlink{PANet}{PANet}]{Path Aggregation Network}
\newacro{mAP}[\AC@hyperlink{mAP}{mAP}]{Mean Average Precision}
\newacro{AP}[\AC@hyperlink{AP}{AP}]{Average Precision}
\newacro{mIoU}[\AC@hyperlink{mIoU}{mIoU}]{Mean Intersection over Union}
\newacro{TP}[\AC@hyperlink{TP}{TP}]{True Positive}
\newacro{TN}[\AC@hyperlink{TN}{TN}]{True Negative}
\newacro{FP}[\AC@hyperlink{FP}{FP}]{False Positive}
\newacro{FN}[\AC@hyperlink{FN}{FN}]{False Negative}
\newacro{FC}[\AC@hyperlink{FC}{FC}]{Fully-Connected}
\newacro{BatchNorm}[\AC@hyperlink{BatchNorm}{BatchNorm}]{Batch Normalization}
\newacro{FCOS}[\AC@hyperlink{FCOS}{FCOS}]{Fully Convolutional One Stage Detector}
\newacro{WBF}[\AC@hyperlink{WBF}{WBF}]{Weighted Bounding Box Fusion}
\newacro{TTA}[\AC@hyperlink{TTA}{TTA}]{Test-Time Augmentation}
\@input{mt-lof.aux}
\@input{mt-lot.aux}
\@input{mt-lit.aux}
\gdef \@abspage@last{58}

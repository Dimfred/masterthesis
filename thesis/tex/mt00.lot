\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {2.1}{\ignorespaces LTspice header syntax\relax }}{4}{}%
\contentsline {table}{\numberline {2.2}{\ignorespaces LTspice symbol names\relax }}{5}{}%
\contentsline {table}{\numberline {2.3}{\ignorespaces LTspice symbol syntax\relax }}{5}{}%
\contentsline {table}{\numberline {2.4}{\ignorespaces LTspice symbol attribute syntax\relax }}{5}{}%
\contentsline {table}{\numberline {2.5}{\ignorespaces LTspice ground syntax\relax }}{6}{}%
\contentsline {table}{\numberline {2.6}{\ignorespaces LTspice wire syntax\relax }}{6}{}%
\contentsline {table}{\numberline {2.7}{\ignorespaces A listing of the different augmentations used from albumentations \cite {albumentation} in this thesis and the target domain where they were applied to.\relax }}{13}{}%
\contentsline {table}{\numberline {2.8}{\ignorespaces The CSPDarknet53Tiny Architecture is a scaled version of the original CSPDarknet53 architecture \cite {yolov4}. The definition for YOLOConv can be found in table 2.9\hbox {}. MaxPool indicates here the Max Pooling operation as described in sec. 2.3.3\hbox {}. The network is build out of five blocks, the first block reducing the image size and the following blocks building up features. Further, features from three different scales are taken and used as skip connections to the following network.\relax }}{20}{}%
\contentsline {table}{\numberline {2.9}{\ignorespaces Convolutional basic building block in YOLOv4 (YOLOConv). A convolutional layer, followed by a batch normalization layer, followed by a leaky ReLU activation function.\relax }}{20}{}%
\contentsline {table}{\numberline {2.10}{\ignorespaces The PANetTiny architecture which is a scaled version of PANet \cite {pannet}, which is the decoder in the YOLO network. It takes as inputs the skip connections from the CSPDarknet53Tiny. The network is build by alternating an output branch and an upsampling branch. First, the most lowest skip connection $Skip_L$ from the backbone is convolved and the output fed into the first output layer. Each output layer has again a $3\times 3$ convolution followed by a $1\times 1$ convolution, which form a raw bounding box prediction, fed to the YOLO head. After an output has been processed the previous convolution is again convolved and upsampled to be fed into the next output block. This is done three times in the whole PANetTiny network.\relax }}{21}{}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Amount of images of \acp {ECD} used in this thesis shown with their underlying background and whether they are annotated or not. Further, the train / validation / test split of the different image types is shown. While this might seem like a big split for test, the number of bounding boxes included in the test set is way smaller and is shown in table 3.2\hbox {}\relax }}{30}{}%
\contentsline {table}{\numberline {3.2}{\ignorespaces The classes present in this thesis with their major class which is an \ac {ECC} and alternatively their orientation. Furthermore, the total amount of classes is shown and the train, valid, test ratio.\relax }}{31}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }

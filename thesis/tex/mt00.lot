\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {2.1}{\ignorespaces LTspice header syntax\relax }}{4}{}%
\contentsline {table}{\numberline {2.2}{\ignorespaces LTspice symbol names\relax }}{5}{}%
\contentsline {table}{\numberline {2.3}{\ignorespaces LTspice symbol syntax\relax }}{5}{}%
\contentsline {table}{\numberline {2.4}{\ignorespaces LTspice symbol attribute syntax\relax }}{5}{}%
\contentsline {table}{\numberline {2.5}{\ignorespaces LTspice ground syntax\relax }}{6}{}%
\contentsline {table}{\numberline {2.6}{\ignorespaces LTspice wire syntax\relax }}{6}{}%
\contentsline {table}{\numberline {2.7}{\ignorespaces A listing of the different augmentations used from albumentations \cite {albumentation} in this thesis and the target domain where they were applied to.\relax }}{14}{}%
\contentsline {table}{\numberline {2.8}{\ignorespaces The CSPDarknet53Tiny Architecture is a scaled version of the original CSPDarknet53 architecture \cite {yolov4}. The definition for YOLOConv can be found in table 2.9\hbox {}. MaxPool indicates here the Max Pooling operation as described in sec. 2.3.3\hbox {}. The network is build out of five blocks, the first block reducing the image size and the following blocks building up features. Further, features from three different scales are taken and used as skip connections to the following network.\relax }}{20}{}%
\contentsline {table}{\numberline {2.9}{\ignorespaces Convolutional basic building block in YOLOv4 (YOLOConv). A convolutional layer, followed by a batch normalization layer, followed by a leaky ReLU activation function.\relax }}{20}{}%
\contentsline {table}{\numberline {2.10}{\ignorespaces The PANetTiny architecture which is a scaled version of PANet \cite {pannet}, which is the decoder in the YOLO network. It takes as inputs the skip connections from the CSPDarknet53Tiny. The network is build by alternating an output branch and an upsampling branch. First, the most lowest skip connection $Skip_L$ from the backbone is convolved and the output fed into the first output layer. Each output layer has again a $3\times 3$ convolution followed by a $1\times 1$ convolution, which form a raw bounding box prediction, fed to the YOLO head. After an output has been processed the previous convolution is again convolved and upsampled to be fed into the next output block. This is done three times in the whole PANetTiny network.\relax }}{22}{}%
\contentsline {table}{\numberline {2.11}{\ignorespaces An inverted residual block transforming from $k$ to $k'$ channels, with stride $s$ and expansion factor $t$.\relax }}{29}{}%
\contentsline {table}{\numberline {2.12}{\ignorespaces MobileNetV2 backbone used in \ac {MUnet} Blocks marked as $Skip_*$ are outputs from the network which are used in the decoder. The parameters above define the configuration of the respective block, $t$ is the expansion factor inside an inverted residual block as defined in 2.11\hbox {}, $k$ is the kernel size for the two standard convolutions used in the backbone, $n$ defines how often that particular block is repeated and $s$ is the stride of a block. Note that if $s = 2$ only the first block has this stride, the others have $s = 1$.\relax }}{30}{}%
\contentsline {table}{\numberline {2.13}{\ignorespaces \ac {MUnet} decoder. The decoder uses transpose convolutions to upsample the input (ConvTranspose) and as the backbone, inverted residuals to process the upsampled input together with the skip connection. The '+' indicates a concatenation along the channel axis. Since the first block in the backbone directly downsamples the input there is no skip connection with the size of the input. Therefore the last layer of the decoder is a upsampling layer, which uses a bilinear upsampling method to increase the size of the prediction to the size of the input. As with the backbone $t$ indicates the expansion size of the inverted residual block, $k$ indicates the used kernel size and $s$ indicates the used stride.\relax }}{31}{}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Amount of images of \acp {ECD} used in this thesis shown with their underlying background and whether they are annotated or not. Further, the train / validation / test split of the different image types is shown. While this might seem like a big split for test, the number of bounding boxes included in the test set is way smaller and is shown in table 3.2\hbox {}\relax }}{36}{}%
\contentsline {table}{\numberline {3.2}{\ignorespaces The classes present in this thesis with their major class which is an \ac {ECC} and alternatively their orientation. Furthermore, the total amount of classes is shown and the train, valid, test ratio.\relax }}{37}{}%
\contentsline {table}{\numberline {3.3}{\ignorespaces The initial training configuration for the experiments performed with the YOLO network.\relax }}{43}{}%
\contentsline {table}{\numberline {3.4}{\ignorespaces Configuration of the YOLO grid search experiment. The networks were trained for all possible configurations of learning rate, batch size and loss function.\relax }}{47}{}%
\contentsline {table}{\numberline {3.5}{\ignorespaces Results of the tuning experiment performed on the validation set, presented also with the performance on the test dataset. A visual representation can also be found in figure 3.9\hbox {}.\relax }}{53}{}%
\contentsline {table}{\numberline {3.6}{\ignorespaces Configuration of the YOLO grid search experiment. The networks were trained for all possible configurations of learning rate, batch size and loss function.\relax }}{53}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {A.1}{\ignorespaces The results of the initial learning rate search shown on the validation set, with mAP over all classes as well as the classwise AP. The mean and standard deviation are taken over three separate runs.\relax }}{78}{}%
\contentsline {table}{\numberline {A.2}{\ignorespaces The results of the offline augmentation configuration search. The configuration should be interpreted in a way that a 1 behind a letter means that particular augmentation is enabled. Further, the letter ``P'' stands for projection, ``F'' for flip and ``R'' means rotation. Rotation always includes a 90\textdegree \, 180\textdegree \ and a 270\textdegree \ rotation. Flip is a horizontal flip. Additionally, when flip and rotation are enabled at the same time the flipped imaged gets also rotated three times. All results are again given with the mean and standard deviation over three runs and the results are compared against the baseline, which is the best performing learning rate from the learning rate search experiment (table A.1\hbox {}).\relax }}{79}{}%
\contentsline {table}{\numberline {A.3}{\ignorespaces The results of the rotation augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{80}{}%
\contentsline {table}{\numberline {A.4}{\ignorespaces The results of the random scale augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{81}{}%
\contentsline {table}{\numberline {A.5}{\ignorespaces The results of the safe bounding box crop augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{82}{}%
\contentsline {table}{\numberline {A.6}{\ignorespaces The results of the color jitter augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{83}{}%

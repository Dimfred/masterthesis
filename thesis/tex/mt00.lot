\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {2.1}{\ignorespaces LTspice header syntax\relax }}{10}{table.2.1}%
\contentsline {table}{\numberline {2.2}{\ignorespaces LTspice symbol names\relax }}{11}{table.2.2}%
\contentsline {table}{\numberline {2.3}{\ignorespaces LTspice symbol syntax\relax }}{11}{table.2.3}%
\contentsline {table}{\numberline {2.4}{\ignorespaces LTspice symbol attribute syntax\relax }}{11}{table.2.4}%
\contentsline {table}{\numberline {2.5}{\ignorespaces LTspice ground syntax\relax }}{12}{table.2.5}%
\contentsline {table}{\numberline {2.6}{\ignorespaces LTspice wire syntax\relax }}{12}{table.2.6}%
\contentsline {table}{\numberline {2.7}{\ignorespaces A listing of the different augmentations used from albumentations \cite {albumentation} in this thesis and the target domain where they were applied to.\relax }}{20}{table.2.7}%
\contentsline {table}{\numberline {2.8}{\ignorespaces Convolutional basic building block in YOLOv4 (YOLOConv). A convolutional layer, followed by a batch normalization layer, followed by a LeakyReLU activation function.\relax }}{26}{table.2.8}%
\contentsline {table}{\numberline {2.9}{\ignorespaces The CSPDarknet53Tiny Architecture is a scaled version of the original CSPDarknet53 architecture \cite {yolov4}. The definition for YOLOConv can be found in table \ref {tab:yoloconv}. MaxPool indicates here the Max Pooling operation as described in sec. \ref {sec:max_pooling}. The network is build out of five blocks, the first block reducing the image size and the following blocks building up features. Further, features from three different scales are taken and used as skip connections to the following network.\relax }}{27}{table.2.9}%
\contentsline {table}{\numberline {2.10}{\ignorespaces The PANetTiny architecture which is a scaled version of PANet \cite {pannet}, which is the decoder in the YOLO network. It takes as inputs the skip connections from the CSPDarknet53Tiny. The network is build by alternating an output branch and an upsampling branch. First, the lowest skip connection $\mathbf {Skip_L}$ from the backbone is convolved and the output fed into the first output layer. Each output layer has again a $3\times 3$ convolution followed by a $1\times 1$ convolution, which form a raw bounding box prediction, fed to the YOLO head. After an output has been processed the previous convolution is again convolved and upsampled to be fed into the next output block. This is done three times in the whole PANetTiny network.\relax }}{28}{table.2.10}%
\contentsline {table}{\numberline {2.11}{\ignorespaces An inverted residual block transforming from $k$ to $k'$ channels, with stride $s$ and expansion factor $t$.\relax }}{35}{table.2.11}%
\contentsline {table}{\numberline {2.12}{\ignorespaces MobileNetV2 encoder used in \ac {MUnet} Blocks marked as $Skip_*$ are outputs from the network which are used in the decoder. The parameters above define the configuration of the respective block, $t$ is the expansion factor inside an inverted residual block as defined in \ref {tab:invres_expansion}, $k$ is the kernel size for the two standard convolutions used in the backbone, $n$ defines how often that particular block is repeated and $s$ is the stride of a block. Note that if $s = 2$ only the first block has this stride, the others have $s = 1$.\relax }}{37}{table.2.12}%
\contentsline {table}{\numberline {2.13}{\ignorespaces \ac {MUnet} decoder. The decoder uses transpose convolutions to upsample the input (ConvTranspose) and as the backbone, inverted residuals to process the upsampled input together with the skip connection. The '+' indicates a concatenation along the channel axis. Since the first block in the backbone directly downsamples the input there is no skip connection with the size of the input. Therefore the last layer of the decoder is a upsampling layer, which uses a bilinear upsampling method to increase the size of the prediction to the size of the input. As with the backbone $t$ indicates the expansion size of the inverted residual block, $k$ indicates the used kernel size and $s$ indicates the used stride.\relax }}{38}{table.2.13}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Amount of images of \acp {ECD} used in this thesis shown with their underlying background and whether they are annotated or not. Further, the train / validation / test split of the different image types is shown. While this might seem like a big split for test, the number of bounding boxes included in the test set is way smaller and is shown in table \ref {tab:yolo_classes}. For the test dataset 10 persons were used, which are not present in the train and validation dataset.\relax }}{44}{table.3.1}%
\contentsline {table}{\numberline {3.2}{\ignorespaces The classes present in this thesis with their major class which is an \ac {ECC} and alternatively their orientation. Furthermore, the total amount of classes is shown and the train, valid, test ratio.\relax }}{45}{table.3.2}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces The initial training configuration for the experiments performed with the YOLO network.\relax }}{57}{table.4.1}%
\contentsline {table}{\numberline {4.2}{\ignorespaces Configuration of the YOLO grid search experiment with the tested grid parameters and the fixed offline and online augmentation parameters. The networks were trained for all possible configurations of learning rate, batch size and loss function. \relax }}{62}{table.4.2}%
\contentsline {table}{\numberline {4.3}{\ignorespaces Results of the tuning experiment with the underlying threshold and input size combination, performed on the validation set, presented also with the performance on the test dataset. A visual representation can also be found in figure \ref {fig:yolo_tuning_combined_results}.\relax }}{66}{table.4.3}%
\contentsline {table}{\numberline {4.4}{\ignorespaces Configuration of the YOLO grid search experiment. The networks were trained for all possible configurations of learning rate, batch size and loss function.\relax }}{68}{table.4.4}%
\contentsline {table}{\numberline {4.5}{\ignorespaces Configuration of the \ac {MUnet} grid search experiment, with tested grid search parameters and fixed offline and offline augmentation parameters. The networks were trained for all possible combinations of learning rate, batch size and loss function.\relax }}{70}{table.4.5}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces The results of the YOLO classification for the tuned networks presented in section \ref {sec:training_yolo}. Classification was done based on a matching \ac {IoU} threshold of 0.3.\relax }}{80}{table.5.1}%
\contentsline {table}{\numberline {5.2}{\ignorespaces Results of the topology evaluation. The focal loss folds all have $\gamma = 2$. The used networks were the ones selected in table \ref {tab:munet_selected_nets}.\relax }}{83}{table.5.2}%
\contentsline {table}{\numberline {5.3}{\ignorespaces Results of the text matching, based on the \ac {YOLO} configurations from table \ref {tab:yolo_classification_res}.\relax }}{84}{table.5.3}%
\contentsline {table}{\numberline {5.4}{\ignorespaces Results of the arrow matching\relax }}{85}{table.5.4}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {A.1}{\ignorespaces The results of the initial learning rate search shown on the validation set, with mAP over all classes as well as the classwise AP. The mean and standard deviation are taken over three separate runs.\relax }}{118}{table.Alph0.1.1}%
\contentsline {table}{\numberline {A.2}{\ignorespaces The results of the offline augmentation configuration search. The letter ``P'' stands for projection, ``F'' for flip and ``R'' means rotation. Rotation always includes a 90\textdegree \, 180\textdegree \ and a 270\textdegree \ rotation. Flip is a horizontal flip. All results are given with the mean and standard deviation over three runs and the results are compared against the baseline, which is the best performing learning rate from the learning rate search experiment (table \ref {tab:yolo_init_lr_results}).\relax }}{119}{table.Alph0.1.2}%
\contentsline {table}{\numberline {A.3}{\ignorespaces The results of the rotation augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{120}{table.Alph0.1.3}%
\contentsline {table}{\numberline {A.4}{\ignorespaces The results of the random scale augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{121}{table.Alph0.1.4}%
\contentsline {table}{\numberline {A.5}{\ignorespaces The results of the safe bounding box crop augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{122}{table.Alph0.1.5}%
\contentsline {table}{\numberline {A.6}{\ignorespaces The results of the color jitter augmentation, performed on different parameters and compared against the best offline augmentation configuration. The results are given with the mean and standard deviation over three runs.\relax }}{123}{table.Alph0.1.6}%
\contentsline {table}{\numberline {B.1}{\ignorespaces The selected \ac {MUnet} folds (combination of batch size and loss) from the grid search experiment. From each fold the best performing metrics were selected, based on the full validation set and the validation set with checkered backgrounds only. The metric values indicate the mean score of that particular fold. From each fold then the best performing training run has been selected as a network for further testing. Overall 36 folds have been selected. Some combinations appeared more then once, hence overall 26 networks have been in the end selected.\relax }}{133}{table.Alph0.2.1}%
